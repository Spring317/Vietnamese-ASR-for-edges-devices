{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a17774b-b4b2-4b00-bb8a-d9f376cd1b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/student5/xuan_quy/asrdata/cache/\n"
     ]
    }
   ],
   "source": [
    "from jupyter_core.paths import jupyter_runtime_dir\n",
    "print(jupyter_runtime_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182a724c-92c3-43c4-9131-235c8949f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 102080])\n",
      "transcript: tướng ca pu chia còn yêu cầu thủ tướng sin ga po phải điều chỉnh phát biểu không đúng sự thật chút nào này theo lợi ông\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/student5/xuan_quy/miniconda3/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:872: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"khanhld/wav2vec2-base-vietnamese-160h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"khanhld/wav2vec2-base-vietnamese-160h\")\n",
    "model.to(device)\n",
    "\n",
    "def transcribe(wav):\n",
    "    input_values = processor(wav, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    print(input_values.shape)\n",
    "    logits = model(input_values.to(device)).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    pred_transcript = processor.batch_decode(pred_ids)[0]\n",
    "    \n",
    "    return pred_transcript\n",
    "\n",
    "\n",
    "wav, _ = librosa.load('/storage/student5/xuan_quy/asrdata/wav/2.wav', sr = 16000)\n",
    "print(f\"transcript: {transcribe(wav)}\")\n",
    "\n",
    "#export model to onnx \n",
    "model = model.cpu()\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 16000)\n",
    "torch.onnx.export(model, dummy_input, \"wav2vec2-base-vietnamese-160h.onnx\", input_names=['input'], output_names=['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45b7e76d-02ab-4db5-b619-c6a3b1ab783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_default_device('cuda:3')\n",
    "print(torch.get_default_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eaac5c-598d-46a7-b4ac-a60ea5f87aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Creating metadata CSV files...\n",
      "Done\n",
      "Load Datset\n",
      "Load audio files\n",
      "Create feature extractor\n",
      "Create processor\n",
      "Process dataset\n",
      "Load pre-trained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/student5/xuan_quy/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_328885/392416954.py:284: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "import json\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from evaluate import load\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from torchsummary import summary\n",
    "# torch.set_default_device('cuda:3')\n",
    "# print(torch.get_default_device())\n",
    "\n",
    "# New function to create metadata CSV files\n",
    "# def create_metadata_csv(audio_dir, transcription_dir, output_dir, test_size=0.2):\n",
    "#     \"\"\"\n",
    "#     Create CSV metadata files from directories containing audio files and transcriptions.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - audio_dir: Directory containing .wav files\n",
    "#     - transcription_dir: Directory containing transcription files\n",
    "#     - output_dir: Directory to save the CSV files\n",
    "#     - test_size: Proportion of data to use for testing\n",
    "#     \"\"\"\n",
    "#     data = []\n",
    "    \n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Iterate through audio files\n",
    "#     for audio_file in Path(audio_dir).glob(\"*.wav\"):\n",
    "#         file_stem = audio_file.stem\n",
    "        \n",
    "#         # Look for corresponding transcription file\n",
    "#         # Assuming transcriptions are in .txt files with same name as audio\n",
    "#         trans_file = Path(transcription_dir) / f\"{file_stem}.txt\"\n",
    "        \n",
    "#         if trans_file.exists():\n",
    "#             # Read transcription\n",
    "#             with open(trans_file, 'r', encoding='utf-8') as f:\n",
    "#                 transcription = f.read().strip()\n",
    "#                 print(f\"Processing {audio_file}: {transcription}\")\n",
    "#             # Verify audio file\n",
    "#             try:\n",
    "#                 waveform, sample_rate = torchaudio.load(str(audio_file))\n",
    "#                 duration = waveform.shape[1] / sample_rate\n",
    "                \n",
    "#                 # Only include files that are valid\n",
    "#                 if duration > 0:\n",
    "#                     data.append({\n",
    "#                         'audio': str(audio_file.absolute()),\n",
    "#                         'text': transcription,\n",
    "#                         'duration': duration\n",
    "#                     })\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {audio_file}: {str(e)}\")\n",
    "#             print(f\"Processed {audio_file}\")    \n",
    "#     # Create DataFrame\n",
    "#     df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Split into train and test sets\n",
    "#     train_df, test_df = train_test_split(\n",
    "#         df, \n",
    "#         test_size=test_size, \n",
    "#         random_state=42\n",
    "#     )\n",
    "    \n",
    "#     # Save CSV files\n",
    "#     train_df.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\n",
    "#     test_df.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n",
    "    \n",
    "#     print(f\"Created metadata files:\")\n",
    "#     print(f\"Train set: {len(train_df)} samples\")\n",
    "#     print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "#     return os.path.join(output_dir, 'train.csv'), os.path.join(output_dir, 'test.csv')\n",
    "\n",
    "# Previous functions remain the same\n",
    "def prepare_vietnamese_text(batch):\n",
    "    text = re.sub(r'[^\\w\\s]', '', batch[\"text\"].lower())\n",
    "    return text\n",
    "\n",
    "def create_vocabulary(dataset):\n",
    "    vocab_dict = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<unk>\": 1,\n",
    "        \"<s>\": 2,\n",
    "        \"</s>\": 3,\n",
    "    }\n",
    "    \n",
    "    chars = set()\n",
    "    for text in dataset[\"text\"]:\n",
    "        chars.update(list(prepare_vietnamese_text({\"text\": text})))\n",
    "    \n",
    "    for i, char in enumerate(sorted(list(chars))):\n",
    "        vocab_dict[char] = i + 4\n",
    "    \n",
    "    return vocab_dict\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    batch[\"input_values\"] = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=True\n",
    "    ).input_values[0]\n",
    "    \n",
    "    batch[\"labels\"] = processor(\n",
    "        text=prepare_vietnamese_text(batch)\n",
    "    ).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "        \n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        labels_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(feat) for feat in label_features],\n",
    "            batch_first=True,\n",
    "            padding_value=-100\n",
    "        )\n",
    "        \n",
    "        batch[\"labels\"] = labels_batch\n",
    "        \n",
    "        return batch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=True)\n",
    "    \n",
    "    wer_metric = load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "def train_vietnamese_asr(audio_dir, transcription_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Main function to prepare data and train the model\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_dir: Directory containing .wav files\n",
    "    - transcription_dir: Directory containing transcription filesload_metric\n",
    "    - output_dir: Directory to save the model and metadata\n",
    "    \"\"\"\n",
    "    # Create metadata CSV files\n",
    "    print(\"Creating metadata CSV files...\")\n",
    "    # train_csv, test_csv = create_metadata_csv(\n",
    "    #     audio_dir=audio_dir,\n",
    "    #     transcription_dir=transcription_dir,\n",
    "    #     output_dir=output_dir\n",
    "    # )\n",
    "    print(\"Done\")\n",
    "    # Load dataset\n",
    "\n",
    "    print(\"Load Datset\")\n",
    "    train_csv ='dataset/train.csv'\n",
    "    test_csv = 'dataset/test.csv'\n",
    "\n",
    "    \n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"csv\", \n",
    "        data_files={\"train\": train_csv, \"test\": test_csv},\n",
    "        delimiter=\",\"\n",
    "    )\n",
    "    \n",
    "    # Load audio files\n",
    "    print('Load audio files')\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "    \n",
    "    # Create vocabulary and save it\n",
    "    # vocab_dict = create_vocabulary(dataset[\"train\"])\n",
    "    # print(vocab_dict)\n",
    "    # vocab_path = os.path.join(output_dir, \"vocab.json\")\n",
    "    \n",
    "    # with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
    "    vocab_path = 'dataset/vocab.json'\n",
    "    tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_path,\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        word_delimiter_token=\"|\"\n",
    "    )\n",
    "    \n",
    "    # Create feature extractor\n",
    "    print('Create feature extractor')\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1,\n",
    "        sampling_rate=16_000,\n",
    "        padding_value=0.0,\n",
    "        do_normalize=True,\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    \n",
    "    # Create processor\n",
    "    print('Create processor')\n",
    "    global processor\n",
    "    processor = Wav2Vec2Processor(\n",
    "        feature_extractor=feature_extractor,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Process dataset\n",
    "    print('Process dataset')\n",
    "    processed_dataset = dataset.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=dataset.column_names[\"train\"],\n",
    "        num_proc=4\n",
    "    )\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    print('Load pre-trained model')\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"facebook/wav2vec2-large-xlsr-53\",\n",
    "        attention_dropout=0.1,\n",
    "        hidden_dropout=0.1,\n",
    "        feat_proj_dropout=0.0,\n",
    "        mask_time_prob=0.05,\n",
    "        layerdrop=0.1,\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "    # print(f\"Model summary: {summary(model)}\")\n",
    "    model.freeze_feature_encoder()\n",
    "    model.to('cuda:0')\n",
    "   \n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(output_dir, \"vietnamese-wav2vec2\"),\n",
    "        dataloader_pin_memory=False,\n",
    "        group_by_length=True,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=10,\n",
    "        fp16=True,\n",
    "        save_steps=500,\n",
    "        eval_steps=500,\n",
    "        logging_steps=500,\n",
    "        learning_rate=1e-4,\n",
    "        warmup_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        gradient_checkpointing=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print('Start training...')\n",
    "    model.gradient_checkpointing_enable()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=DataCollatorCTCWithPadding(processor=processor, padding=True),\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=processed_dataset[\"train\"],\n",
    "        eval_dataset=processed_dataset[\"test\"],\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model(os.path.join(output_dir, \"vietnamese-wav2vec2-final\"))\n",
    "    print(\"Completed training\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting...\")\n",
    "    train_vietnamese_asr(\n",
    "        audio_dir=\"wav\",\n",
    "        transcription_dir=\"txt\",\n",
    "        output_dir=\"dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acce3c6-976f-44a7-a7ae-ef7e8c849d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
