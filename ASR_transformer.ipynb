{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a17774b-b4b2-4b00-bb8a-d9f376cd1b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/student5/xuan_quy/asrdata/cache/\n"
     ]
    }
   ],
   "source": [
    "from jupyter_core.paths import jupyter_runtime_dir\n",
    "print(jupyter_runtime_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182a724c-92c3-43c4-9131-235c8949f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 94445536\n",
      "torch.Size([1, 102080])\n",
      "transcript: tướng ca pu chia còn yêu cầu thủ tướng sin ga po phải điều chỉnh phát biểu không đúng sự thật chút nào này theo lợi ông\n",
      "True: tướng cam pu chia còn yêu cầu thủ tướng sinh ga po phải điều chỉnh phát biểu không đúng sự thật chút nào này theo lời ông\n",
      "torch.Size([1, 102080])\n",
      "Score: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import librosa\n",
    "import torch\n",
    "from jiwer import wer \n",
    "device = torch.device(\"cuda:3\")\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"khanhld/wav2vec2-base-vietnamese-160h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"khanhld/wav2vec2-base-vietnamese-160h\")\n",
    "model.to(device)\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "def transcribe(wav):\n",
    "    input_values = processor(wav, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    print(input_values.shape)\n",
    "    logits = model(input_values.to(device)).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    pred_transcript = processor.batch_decode(pred_ids)[0]\n",
    "    \n",
    "    return pred_transcript\n",
    "\n",
    "\n",
    "wav, _ = librosa.load('/storage/student5/xuan_quy/asrdata/wav/2.wav', sr = 16000)\n",
    "print(f\"transcript: {transcribe(wav)}\")\n",
    "with open(\"txt/2.txt\",\"r\") as f:\n",
    "    true = f.read()\n",
    "    print(f\"True: {true}\")\n",
    "print(f\"Score: {wer(transcribe(wav), true)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded908c1-9711-4973-910a-3cf5a1ea214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "# import onnxruntime as ort\n",
    "def export(model):\n",
    "    \"\"\"Export model into onnx format\"\"\"\n",
    "    dummy_input = torch.randn(1, 16000)\n",
    "    input_names = [\"audio\"]\n",
    "    output_names = [\"text\"]\n",
    "    torch.onnx.export(model, dummy_input, \"wav2vec2-base-vietnamese-160h.onnx\", input_names=input_names, output_names=output_names, opset_version=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b7e76d-02ab-4db5-b619-c6a3b1ab783b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_default_device('cuda:3')\n",
    "print(torch.get_default_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05eaac5c-598d-46a7-b4ac-a60ea5f87aa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n",
      "Starting...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Wav2Vec2ForCTC:\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([96, 768]) from checkpoint, the shape in current model is torch.Size([98, 768]).\n\tsize mismatch for lm_head.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([98]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 281\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 281\u001b[0m     \u001b[43mtrain_vietnamese_asr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43maudio_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtranscription_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtxt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 236\u001b[0m, in \u001b[0;36mtrain_vietnamese_asr\u001b[0;34m(audio_dir, transcription_dir, output_dir)\u001b[0m\n\u001b[1;32m    229\u001b[0m processed_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    230\u001b[0m     prepare_dataset,\n\u001b[1;32m    231\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    232\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    233\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Load pre-trained model\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2ForCTC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkhanhld/wav2vec2-base-vietnamese-160h\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeat_proj_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_time_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayerdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctc_loss_reduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# print(f\"Model summary: {summary(model)}\")\u001b[39;00m\n\u001b[1;32m    248\u001b[0m model\u001b[38;5;241m.\u001b[39mfreeze_feature_encoder()\n",
      "File \u001b[0;32m/storage/student5/xuan_quy/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4225\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4216\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4218\u001b[0m     (\n\u001b[1;32m   4219\u001b[0m         model,\n\u001b[1;32m   4220\u001b[0m         missing_keys,\n\u001b[1;32m   4221\u001b[0m         unexpected_keys,\n\u001b[1;32m   4222\u001b[0m         mismatched_keys,\n\u001b[1;32m   4223\u001b[0m         offload_index,\n\u001b[1;32m   4224\u001b[0m         error_msgs,\n\u001b[0;32m-> 4225\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4232\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4237\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4245\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4246\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/storage/student5/xuan_quy/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4785\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize mismatch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg:\n\u001b[1;32m   4782\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   4783\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4784\u001b[0m         )\n\u001b[0;32m-> 4785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4788\u001b[0m     archs \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39marchitectures\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Wav2Vec2ForCTC:\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([96, 768]) from checkpoint, the shape in current model is torch.Size([98, 768]).\n\tsize mismatch for lm_head.bias: copying a param with shape torch.Size([96]) from checkpoint, the shape in current model is torch.Size([98]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "import json\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from evaluate import load\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from torchsummary import summary\n",
    "torch.set_default_device('cuda:3')\n",
    "print(torch.get_default_device())\n",
    "\n",
    "# New function to create metadata CSV files\n",
    "# def create_metadata_csv(audio_dir, transcription_dir, output_dir, test_size=0.2):\n",
    "#     \"\"\"\n",
    "#     Create CSV metadata files from directories containing audio files and transcriptions.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - audio_dir: Directory containing .wav files\n",
    "#     - transcription_dir: Directory containing transcription files\n",
    "#     - output_dir: Directory to save the CSV files\n",
    "#     - test_size: Proportion of data to use for testing\n",
    "#     \"\"\"\n",
    "#     data = []\n",
    "    \n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Iterate through audio files\n",
    "#     for audio_file in Path(audio_dir).glob(\"*.wav\"):\n",
    "#         file_stem = audio_file.stem\n",
    "        \n",
    "#         # Look for corresponding transcription file\n",
    "#         # Assuming transcriptions are in .txt files with same name as audio\n",
    "#         trans_file = Path(transcription_dir) / f\"{file_stem}.txt\"\n",
    "        \n",
    "#         if trans_file.exists():\n",
    "#             # Read transcription\n",
    "#             with open(trans_file, 'r', encoding='utf-8') as f:\n",
    "#                 transcription = f.read().strip()\n",
    "#                 print(f\"Processing {audio_file}: {transcription}\")\n",
    "#             # Verify audio file\n",
    "#             try:\n",
    "#                 waveform, sample_rate = torchaudio.load(str(audio_file))\n",
    "#                 duration = waveform.shape[1] / sample_rate\n",
    "                \n",
    "#                 # Only include files that are valid\n",
    "#                 if duration > 0:\n",
    "#                     data.append({\n",
    "#                         'audio': str(audio_file.absolute()),\n",
    "#                         'text': transcription,\n",
    "#                         'duration': duration\n",
    "#                     })\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing {audio_file}: {str(e)}\")\n",
    "#             print(f\"Processed {audio_file}\")    \n",
    "#     # Create DataFrame\n",
    "#     df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Split into train and test sets\n",
    "#     train_df, test_df = train_test_split(\n",
    "#         df, \n",
    "#         test_size=test_size, \n",
    "#         random_state=42\n",
    "#     )\n",
    "    \n",
    "#     # Save CSV files\n",
    "#     train_df.to_csv(os.path.join(output_dir, 'train.csv'), index=False)\n",
    "#     test_df.to_csv(os.path.join(output_dir, 'test.csv'), index=False)\n",
    "    \n",
    "#     print(f\"Created metadata files:\")\n",
    "#     print(f\"Train set: {len(train_df)} samples\")\n",
    "#     print(f\"Test set: {len(test_df)} samples\")\n",
    "    \n",
    "#     return os.path.join(output_dir, 'train.csv'), os.path.join(output_dir, 'test.csv')\n",
    "\n",
    "# Previous functions remain the same\n",
    "def prepare_vietnamese_text(batch):\n",
    "    text = re.sub(r'[^\\w\\s]', '', batch[\"text\"].lower())\n",
    "    return text\n",
    "\n",
    "def create_vocabulary(dataset):\n",
    "    vocab_dict = {\n",
    "        \"<pad>\": 0,\n",
    "        \"<unk>\": 1,\n",
    "        \"<s>\": 2,\n",
    "        \"</s>\": 3,\n",
    "    }\n",
    "    \n",
    "    chars = set()\n",
    "    for text in dataset[\"text\"]:\n",
    "        chars.update(list(prepare_vietnamese_text({\"text\": text})))\n",
    "    \n",
    "    for i, char in enumerate(sorted(list(chars))):\n",
    "        vocab_dict[char] = i + 4\n",
    "    \n",
    "    return vocab_dict\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    batch[\"input_values\"] = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=True\n",
    "    ).input_values[0]\n",
    "    \n",
    "    batch[\"labels\"] = processor(\n",
    "        text=prepare_vietnamese_text(batch)\n",
    "    ).input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "        \n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        labels_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(feat) for feat in label_features],\n",
    "            batch_first=True,\n",
    "            padding_value=-100\n",
    "        )\n",
    "        \n",
    "        batch[\"labels\"] = labels_batch\n",
    "        \n",
    "        return batch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=True)\n",
    "    \n",
    "    wer_metric = load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "def train_vietnamese_asr(audio_dir, transcription_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Main function to prepare data and train the model\n",
    "    \n",
    "    Parameters:\n",
    "    - audio_dir: Directory containing .wav files\n",
    "    - transcription_dir: Directory containing transcription filesload_metric\n",
    "    - output_dir: Directory to save the model and metadata\n",
    "    \"\"\"\n",
    "    # Create metadata CSV files\n",
    "    # print(\"Creating metadata CSV files...\")\n",
    "    # train_csv, test_csv = create_metadata_csv(\n",
    "    #     audio_dir=audio_dir,\n",
    "    #     transcription_dir=transcription_dir,\n",
    "    #     output_dir=output_dir\n",
    "    # )\n",
    "    \n",
    "    # Load dataset\n",
    "    train_csv ='dataset/train.csv'\n",
    "    test_csv = 'dataset/test.csv'\n",
    "\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"csv\", \n",
    "        data_files={\"train\": train_csv, \"test\": test_csv},\n",
    "        delimiter=\",\"\n",
    "    )\n",
    "    \n",
    "    # Load audio files\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "    \n",
    "    # Create vocabulary and save it\n",
    "    # vocab_dict = create_vocabulary(dataset[\"train\"])\n",
    "    # print(vocab_dict)\n",
    "    # vocab_path = os.path.join(output_dir, \"vocab.json\")\n",
    "    \n",
    "    # with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(vocab_dict, f, ensure_ascii=False, indent=2)\n",
    "    vocab_path = 'dataset/vocab.json'\n",
    "    tokenizer = Wav2Vec2CTCTokenizer(\n",
    "        vocab_path,\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        word_delimiter_token=\"|\"\n",
    "    )\n",
    "    \n",
    "    # Create feature extractor\n",
    "    feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "        feature_size=1,\n",
    "        sampling_rate=16_000,\n",
    "        padding_value=0.0,\n",
    "        do_normalize=True,\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    \n",
    "    # Create processor\n",
    "    global processor\n",
    "    processor = Wav2Vec2Processor(\n",
    "        feature_extractor=feature_extractor,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Process dataset\n",
    "    processed_dataset = dataset.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=dataset.column_names[\"train\"],\n",
    "        num_proc=4\n",
    "    )\n",
    "    \n",
    "    # Load pre-trained model\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\n",
    "        \"khanhld/wav2vec2-base-vietnamese-160h\",\n",
    "        attention_dropout=0.1,\n",
    "        hidden_dropout=0.1,\n",
    "        feat_proj_dropout=0.0,\n",
    "        mask_time_prob=0.05,\n",
    "        layerdrop=0.1,\n",
    "        ctc_loss_reduction=\"mean\",\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        vocab_size=len(processor.tokenizer)\n",
    "    )\n",
    "    # print(f\"Model summary: {summary(model)}\")\n",
    "    model.freeze_feature_encoder()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(output_dir, \"vietnamese-wav2vec2\"),\n",
    "        group_by_length=True,\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=30,\n",
    "        fp16=True,\n",
    "        save_steps=500,\n",
    "        eval_steps=500,\n",
    "        logging_steps=500,\n",
    "        learning_rate=1e-4,\n",
    "        warmup_steps=1000,\n",
    "        save_total_limit=2\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        data_collator=DataCollatorCTCWithPadding(processor=processor, padding=True),\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=processed_dataset[\"train\"],\n",
    "        eval_dataset=processed_dataset[\"test\"],\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(os.path.join(output_dir, \"vietnamese-wav2vec2-final\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting...\")\n",
    "    train_vietnamese_asr(\n",
    "        audio_dir=\"wav\",\n",
    "        transcription_dir=\"txt\",\n",
    "        output_dir=\"dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8acce3c6-976f-44a7-a7ae-ef7e8c849d9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'mozilla-foundation/common_voice_8_0' is a gated dataset on the Hub. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmozilla-foundation/common_voice_8_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_GMWiGfcqLcBDpanrjvVDCXyskwWSJZCZQc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mcast_column(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, Audio(sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m))\n\u001b[1;32m     20\u001b[0m chars_to_ignore \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[,?.!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-;:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m“\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m�]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# ignore special characters\u001b[39;00m\n",
      "File \u001b[0;32m/storage/student5/xuan_quy/miniconda3/lib/python3.12/site-packages/datasets/load.py:2132\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2128\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2129\u001b[0m )\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/storage/student5/xuan_quy/miniconda3/lib/python3.12/site-packages/datasets/load.py:1853\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1852\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mstorage_options\u001b[38;5;241m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1853\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_custom_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/storage/student5/xuan_quy/miniconda3/lib/python3.12/site-packages/datasets/load.py:1717\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/storage/student5/xuan_quy/miniconda3/lib/python3.12/site-packages/datasets/load.py:1701\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m   1700\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Visit the dataset page at https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to ask for access.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1704\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1705\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'mozilla-foundation/common_voice_8_0' is a gated dataset on the Hub. You must be authenticated to access it."
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset, Audio\n",
    "import evaluate\n",
    "\n",
    "wer = evaluate.load('wer')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"khanhld/wav2vec2-base-vietnamese-160h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"khanhld/wav2vec2-base-vietnamese-160h\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"vi\", split=\"test\", use_auth_token=\"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\")\n",
    "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "chars_to_ignore = r'[,?.!\\-;:\"“%\\'�]' # ignore special characters\n",
    "\n",
    "# preprocess data\n",
    "def preprocess(batch):\n",
    "  audio = batch[\"audio\"]\n",
    "  batch[\"input_values\"] = audio[\"array\"]\n",
    "  batch[\"transcript\"] = re.sub(chars_to_ignore, '', batch[\"sentence\"]).lower()\n",
    "  return batch\n",
    "\n",
    "# run inference\n",
    "def inference(batch):\n",
    "  input_values = processor(batch[\"input_values\"], \n",
    "                            sampling_rate=16000, \n",
    "                            return_tensors=\"pt\").input_values\n",
    "  logits = model(input_values.to(device)).logits\n",
    "  pred_ids = torch.argmax(logits, dim=-1)\n",
    "  batch[\"pred_transcript\"] = processor.batch_decode(pred_ids) \n",
    "  return batch\n",
    "  \n",
    "test_dataset = test_dataset.map(preprocess)\n",
    "result = test_dataset.map(inference, batched=True, batch_size=1)\n",
    "print(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_transcript\"], references=result[\"transcript\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
