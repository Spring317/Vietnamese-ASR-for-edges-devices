{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/spring/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/home/spring/miniconda3/lib/python3.12/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:872: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n",
      "\t\t-[x] values not close enough, max diff: 8.33272933959961e-05 (atol: 1e-05)\n",
      "The ONNX export succeeded with the warning: The maximum absolute difference between the output of the reference model and the ONNX exported model is not within the set tolerance 1e-05:\n",
      "- logits: max diff = 8.33272933959961e-05.\n",
      " The exported model was saved at: w2v2_onnx\n"
     ]
    }
   ],
   "source": [
    "!optimum-cli export onnx --model khanhld/wav2vec2-base-vietnamese-160h w2v2_onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spring/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'optimum.onnxruntime.modeling_ort.ORTModelForCTC'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from optimum.onnxruntime import ORTModelForCTC, ORTModelForSpeechSeq2Seq\n",
    "from jiwer import wer \n",
    "model = ORTModelForCTC.from_pretrained(\"w2v2_onnx/\")\n",
    "print(type(model))\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"khanhld/wav2vec2-base-vietnamese-160h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 102080])\n",
      "predicted:tướng ca pu chia còn yêu cầu thủ tướng sin ga po phải điều chỉnh phát biểu không đúng sự thật chút nào này theo lợi ông\n",
      "ground truth:tướng cam pu chia còn yêu cầu thủ tướng sinh ga po phải điều chỉnh phát biểu không đúng sự thật chút nào này theo lời ông\n",
      "WER:0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test model\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def load_audio(file_path):\n",
    "    speech_array, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "    inputs = processor(speech_array, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True).input_values\n",
    "    return inputs\n",
    "\n",
    "def transcribe(file_path):\n",
    "    input_values = load_audio(file_path)\n",
    "    print(input_values.shape)\n",
    "    logits = model(input_values).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    pred_transcript = processor.batch_decode(pred_ids)[0]\n",
    "    \n",
    "    return pred_transcript\n",
    "\n",
    "\n",
    "preds = transcribe(\"wav/2.wav\")\n",
    "print(f\"predicted:{preds}\")\n",
    "with open('txt/2.txt', 'r') as file:\n",
    "    true = file.read()\n",
    "    print(f\"ground truth:{true}\")\n",
    "print(f\"WER:{wer(preds, true)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 139200])\n",
      "processing time:0.7745380401611328\n",
      "transcribe: hôm they hôm phu có nghĩa là chưa có cái giấy hôm thú nếu mà vậy thì cái cuộc bở lãnh phải làm coi như là cái người mà được ba lãnh họ phả ở nước ngoài học kế họ không\n",
      "torch.Size([1, 50080])\n",
      "processing time:0.20462536811828613\n",
      "transcribe: theo bà đây là một sự thật đã được thừa nhận rộng rãi\n",
      "torch.Size([1, 102080])\n",
      "processing time:0.4502291679382324\n",
      "transcribe: tướng ca pu chia còn yêu cầu thủ tướng sin ga po phải điều chỉnh phát biểu không đúng sự thật chút nào này theo lợi ông\n",
      "torch.Size([1, 180320])\n",
      "processing time:0.7073523998260498\n",
      "transcribe: nó làm người dân mặc dù nhìn thủ rất bình thường nhớg mà rất rất là rộng ràng à thừ  từ từ tính tác đường phố và trên mỗi con mỗi con ngườn\n",
      "torch.Size([1, 98240])\n",
      "processing time:0.36347389221191406\n",
      "transcribe: bây giờ mình nói từ khe xanh hai năm tới thể xanh vĩnh viễn ở mình đang ờ nói vì thẻ xanh nhưng mà có quá nhiều câu hỏi\n",
      "torch.Size([1, 151360])\n",
      "processing time:0.593306303024292\n",
      "transcribe: mọi người đã thử các kiểu ăn kiêng và luyện tập khác nhau mà không hiệu quả họ thấy nếy kuật như một cách để thấy được sự rõ ràng hơn về những gì đang thay đổi trên cơ thể của họ\n",
      "torch.Size([1, 215040])\n",
      "processing time:0.9249684810638428\n",
      "transcribe: khi chúng tôi về tến đó thì thấy thần tôi ồichúng tôi nghì van mà cần thầy có cái tự an toàn tôi người dân và tên hết hai nơi thì chúng tôi cũng nhận được ái từ góp ý bổ an nhen linh mục và của bề tên  chúng tôi  tình về cho ba con ra vêạ\n",
      "torch.Size([1, 158400])\n",
      "processing time:0.6338121891021729\n",
      "transcribe: bà cho biết cuộc đua của bà đang diễn ra gây go vì số phiếu khiếm diện gửi lại không nhiều như trong đợi và bà đang nỗ lực hết sức để huy động nhiều phiếu của cự trì gốc việc nhất có thể\n",
      "torch.Size([1, 181440])\n",
      "processing time:0.7345612049102783\n",
      "transcribe: nó côn nếu cơm đạo bạo thì mình cứ nấy ái gạo nó ngon ngon một chút ì về mình nấu cơm nó ngon nên thì người ta mua thì một nàn hai nàn tớ thấy ngon người tấ cứ tới ta mua hoài dậy á\n",
      "torch.Size([1, 296800])\n",
      "processing time:1.3046460151672363\n",
      "transcribe: guý thứ hai vừa qua nhà chức chác đại khu vực này đã công khai kế hoạch củbổ sung thêm những đơn vị cảnh sát đi bộ bi tuần trên những còn phố có tỷ lệ tội phạm cao giống như a gài đây là một bức tiền mới nhằm đối phó với tình trạng mất crật tự án ninh và buôn ban ma túý đại đầy chỉ như ý một thủ tiệm trong khu vực cho biết\n"
     ]
    }
   ],
   "source": [
    "#evaluate processing time\n",
    "import time\n",
    "times = []\n",
    "for i in range(10):\n",
    "    start = time.time()\n",
    "    preds = transcribe(f\"wav/{i}.wav\")\n",
    "    print(f\"processing time:{time.time()-start}\")\n",
    "    print(f\"transcribe: {preds}\")\n",
    "    end = time.time()\n",
    "    times.append(end-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Tried to use ORTOptimizer for the model type wav2vec2, but it is not available yet. Please open an issue or submit a PR at https://github.com/huggingface/optimum.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/optimum/onnxruntime/optimization.py:68\u001b[0m, in \u001b[0;36mORTOptimizer.__init__\u001b[0;34m(self, onnx_model_path, config, from_ortmodel)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_config \u001b[38;5;241m=\u001b[39m \u001b[43mNormalizedConfigManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_normalized_config_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/optimum/utils/normalized_config.py:303\u001b[0m, in \u001b[0;36mNormalizedConfigManager.get_normalized_config_class\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m    302\u001b[0m model_type \u001b[38;5;241m=\u001b[39m model_type\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_supported_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_conf[model_type]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/optimum/utils/normalized_config.py:295\u001b[0m, in \u001b[0;36mNormalizedConfigManager.check_supported_model\u001b[0;34m(cls, model_type)\u001b[0m\n\u001b[1;32m    294\u001b[0m model_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m model type is not supported yet in NormalizedConfig. Only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want to support \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m please propose a PR or open up an issue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'wav2vec2 model type is not supported yet in NormalizedConfig. Only albert, bart, bert, blenderbot, blenderbot-small, bloom, falcon, camembert, codegen, cvt, deberta, deberta-v2, deit, distilbert, donut-swin, electra, encoder-decoder, gemma, gpt2, gpt-bigcode, gpt-neo, gpt-neox, gptj, imagegpt, llama, longt5, marian, markuplm, mbart, mistral, mixtral, mpnet, mpt, mt5, m2m-100, nystromformer, opt, pegasus, pix2struct, phi, phi3, phi3small, poolformer, regnet, resnet, roberta, segformer, speech-to-text, splinter, t5, trocr, vision-encoder-decoder, vit, whisper, xlm-roberta, yolos, qwen2, granite are supported. If you want to support wav2vec2 please propose a PR or open up an issue.'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m processor \u001b[38;5;241m=\u001b[39m Wav2Vec2Processor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkhanhld/wav2vec2-base-vietnamese-160h\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m ORTModelForCTC\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw2v2_onnx/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mORTOptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m save_dir_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2v2_optim1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m save_dir_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2v2_optim2\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/optimum/onnxruntime/optimization.py:120\u001b[0m, in \u001b[0;36mORTOptimizer.from_pretrained\u001b[0;34m(cls, model_or_path, file_names)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load the model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_ortmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_ortmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/optimum/onnxruntime/optimization.py:70\u001b[0m, in \u001b[0;36mORTOptimizer.__init__\u001b[0;34m(self, onnx_model_path, config, from_ortmodel)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_config \u001b[38;5;241m=\u001b[39m NormalizedConfigManager\u001b[38;5;241m.\u001b[39mget_normalized_config_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to use ORTOptimizer for the model type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but it is not available yet. Please open an issue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or submit a PR at https://github.com/huggingface/optimum.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Tried to use ORTOptimizer for the model type wav2vec2, but it is not available yet. Please open an issue or submit a PR at https://github.com/huggingface/optimum."
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer, ORTModelForCTC, AutoOptimizationConfig\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"khanhld/wav2vec2-base-vietnamese-160h\")\n",
    "model = ORTModelForCTC.from_pretrained(\"w2v2_onnx/\")\n",
    "\n",
    "optimizer = ORTOptimizer.from_pretrained(model)\n",
    "save_dir_1 = 'w2v2_optim1'\n",
    "save_dir_2 = 'w2v2_optim2'\n",
    "save_dir_3 = 'w2v2_optim3'\n",
    "\n",
    "optimization_config_1 = AutoOptimizationConfig.O1()\n",
    "optimizer.optimize(save_dir=save_dir_1, optimization_config=optimization_config_1)\n",
    "\n",
    "optimization_config_2 = AutoOptimizationConfig.O2()\n",
    "optimizer.optimize(save_dir=save_dir_2, optimization_config=optimization_config_2)\n",
    "\n",
    "optimization_config_3 = AutoOptimizationConfig.O3()\n",
    "optimizer.optimize(save_dir=save_dir_3, optimization_config=optimization_config_3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give TVM a shot cuz why not? (NVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "import tvm.relay as relay\n",
    "import tvm\n",
    "from tvm.contrib import graph_executor\n",
    "import librosa\n",
    "#load w2v2_onnx model\n",
    "model_path = \"w2v2_onnx/model.onnx\"\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spring/tvm/python/tvm/relay/frontend/onnx.py:7234: UserWarning: 'ORTModelForCTC' object has no attribute 'SerializeToString'\n",
      "  warnings.warn(str(e))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ORTModelForCTC' object has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m input_data \u001b[38;5;241m=\u001b[39m load_audio(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav/2.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m shape_dict \u001b[38;5;241m=\u001b[39m {input_name: [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]}\n\u001b[0;32m----> 5\u001b[0m mod, params \u001b[38;5;241m=\u001b[39m \u001b[43mrelay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrontend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tvm\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mPassContext(opt_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      8\u001b[0m     lib \u001b[38;5;241m=\u001b[39m relay\u001b[38;5;241m.\u001b[39mbuild(mod, target, params\u001b[38;5;241m=\u001b[39mparams)\n",
      "File \u001b[0;32m~/tvm/python/tvm/relay/frontend/onnx.py:7238\u001b[0m, in \u001b[0;36mfrom_onnx\u001b[0;34m(model, shape, dtype, opset, freeze_params, convert_config, export_node_renamed_model_path)\u001b[0m\n\u001b[1;32m   7236\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   7237\u001b[0m g \u001b[38;5;241m=\u001b[39m GraphProto(shape, dtype, freeze_params, op_type_dict\u001b[38;5;241m=\u001b[39m{})\n\u001b[0;32m-> 7238\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\n\u001b[1;32m   7240\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   7241\u001b[0m     opset_in_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ORTModelForCTC' object has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "target = \"llvm\"\n",
    "input_name = 'input'\n",
    "input_data = load_audio(\"wav/2.wav\")\n",
    "shape_dict = {input_name: [0,0]}\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n",
    "\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target, params=params)\n",
    "\n",
    "dev = tvm.device(str(target), 0)\n",
    "module = graph_executor.GraphModule(lib[\"default\"](dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name: input_values\n",
      "Input shape: [0, 0]\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load your ONNX model\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "# Print the input names\n",
    "for input in onnx_model.graph.input:\n",
    "    print(f\"Input name: {input.name}\")\n",
    "    print(f\"Input shape: {[dim.dim_value for dim in input.type.tensor_type.shape.dim]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
