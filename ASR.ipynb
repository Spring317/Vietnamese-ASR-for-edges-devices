{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8oDo0gWYU5MM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import os \n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import os\n",
    "# from transformers import AutoTokenizer\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import Dataset\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_folder_path = \"/storage/student5/xuan_quy/asrdata/txt\"\n",
    "\n",
    "# # Define a temporary file to combine the inputs\n",
    "# combined_text_path = \"combined_vietnamese_text.txt\"\n",
    "\n",
    "# # Define the path to save your trained BPE model\n",
    "# bpe_model_path = \"vietnamese_bpe.model\"\n",
    "\n",
    "# vocab_size = 8000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Path to the folder containing your Vietnamese text files\n",
    "# input_folder_path = \"/storage/student5/xuan_quy/asrdata/txt\"\n",
    "\n",
    "# # Define a temporary file to combine the inputs\n",
    "# combined_text_path = \"combined_vietnamese_text.txt\"\n",
    "\n",
    "# # Define the path to save your trained BPE model\n",
    "# bpe_model_path = \"vietnamese_bpe.model\"\n",
    "\n",
    "# # Set the desired vocabulary size\n",
    "# vocab_size = 8000  # Adjust this based on your requirements\n",
    "\n",
    "# # Combine all text files in the folder into one temporary file\n",
    "# with open(combined_text_path, \"w\") as outfile:\n",
    "#     for file_path in Path(input_folder_path).glob(\"*.txt\"):\n",
    "#         with open(file_path, \"r\") as infile:\n",
    "#             outfile.write(infile.read() + \"\\n\")\n",
    "\n",
    "# # Train the BPE model using the combined file\n",
    "# yttm.BPE.train(data=combined_text_path, vocab_size=vocab_size, model=bpe_model_path)\n",
    "\n",
    "# # Optionally remove the temporary combined file\n",
    "# os.remove(combined_text_path)\n",
    "\n",
    "# print(f\"BPE model trained and saved at: {bpe_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the trained BPE model\n",
    "\n",
    "\n",
    "# # Example Vietnamese sentence\n",
    "# sentence = \"Tôi muốn học máy học và trí tuệ nhân tạo.\"\n",
    "\n",
    "# # Encode the sentence to BPE tokens\n",
    "# encoded = bpe.encode(sentence, output_type=yttm.OutputType.ID)\n",
    "# print(\"Encoded IDs:\", encoded)\n",
    "\n",
    "# # Encode the sentence to BPE subword tokens\n",
    "# subwords = bpe.encode(sentence, output_type=yttm.OutputType.SUBWORD)\n",
    "# print(\"Encoded Subwords:\", subwords)\n",
    "\n",
    "# # Decode the encoded IDs back to the original sentence\n",
    "# decoded = bpe.decode(encoded)\n",
    "# print(\"Decoded Sentence:\", decoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "# from pathlib import Path\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def encode_text_file(file_path, bpe):\n",
    "#     with open(file_path, \"r\") as f:\n",
    "#         text = f.read().strip()\n",
    "#     # Encode the text to a list of subword IDs\n",
    "#     token_ids = bpe.encode(text, output_type=yttm.OutputType.ID)\n",
    "#     return torch.tensor(token_ids, dtype=torch.long)\n",
    " \n",
    " \n",
    "# bpe = yttm.BPE(model=bpe_model_path)\n",
    "# # Collect and encode all files\n",
    "# encoded_texts = []\n",
    "# for i in range(len(os.listdir(input_folder_path))):\n",
    "#     encoded_texts.append(encode_text_file(f'{input_folder_path}/{i}.txt', bpe))\n",
    "#     print(f\"Encoded {i}.txt\")\n",
    "\n",
    "# # Pad encoded texts to the length of the longest sequence\n",
    "# padded_encoded_texts = pad_sequence(encoded_texts, batch_first=True, padding_value= vocab_size+1)\n",
    "# torch.save(padded_encoded_texts, \"padded_encoded_texts.pt\")\n",
    "# print(\"Padded Encoded Texts:\")\n",
    "# print(padded_encoded_texts)\n",
    "# print(\"Shape:\", padded_encoded_texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_file_path = \"/storage/student5/xuan_quy/asrdata/wav/\"\n",
    "# from pathlib import Path\n",
    "\n",
    "# train = []\n",
    "# def extract_audio_features(file_path):\n",
    "#     # Load the audio file\n",
    "#     waveform, sample_rate = torchaudio.load(file_path)\n",
    "#     spectrogram = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft= 1024,n_mels=64)(waveform)\n",
    "#     return spectrogram.squeeze(0).transpose(0, 1)\n",
    "\n",
    "# # audio = extract_audio_features(audio_file_path)\n",
    "# # print(audio.shape)\n",
    "# for i in range(len(os.listdir(audio_file_path))):\n",
    "    \n",
    "#     audio = extract_audio_features(f'{audio_file_path}/{i}.wav')\n",
    "#     print(f\"Processed audio file: {i}.wav\")\n",
    "#     print(\"Shape:\", audio.shape)\n",
    "#     print(\"\")\n",
    "#     train.append(audio)\n",
    "\n",
    "# # Pad the audio features to the length of the longest sequence\n",
    "# padded_audio = pad_sequence(train, batch_first=True, padding_value=0)\n",
    "\n",
    "# print(\"Padded Audio Features:\")\n",
    "# print(padded_audio)\n",
    "# print(\"Shape:\", padded_audio.shape)\n",
    "# torch.save(padded_audio, \"padded_audio.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform, sr = torchaudio.load(\"/storage/student5/xuan_quy/asrdata/wav/1991.wav\")\n",
    "# print(waveform.shape)\n",
    "# melspec = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft= 1024,n_mels=64)(waveform)\n",
    "# print(melspec.shape)\n",
    "# print(melspec.squeeze(0).transpose(0, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# wav_folder = '/storage/student5/xuan_quy/asrdata/wav'\n",
    "# txt_folder = \"/storage/student5/xuan_quy/asrdata/txt\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "\n",
    "# # Prepare list to store audio data and tokenized transcriptions\n",
    "# data = {\"audio\": [], \"text\": [], \"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "# # Loop through all .wav and .txt files in the folders\n",
    "# for filename in os.listdir(wav_folder):\n",
    "#     if filename.endswith(\".wav\"):\n",
    "#         file_path = os.path.join(wav_folder, filename)\n",
    "#         print(f\"Processing audio file {file_path}\")\n",
    "#         # Load audio file\n",
    "#         waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "#         # Resample if not at 16 kHz (Wav2Vec2 expects 16kHz audio)\n",
    "#         if sample_rate != 16000:\n",
    "#             resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "#             waveform = resampler(waveform)\n",
    "#         #add padding\n",
    "#         max_len = 16000\n",
    "#         waveform = F.pad(waveform, (0, max_len - waveform.shape[1]), \"constant\", 0)\n",
    "#         # Convert waveform to a 1D tensor\n",
    "#         waveform = waveform.squeeze(0)\n",
    "#         print(waveform.shape)\n",
    "#         # Add waveform data to dataset\n",
    "#         data[\"audio\"].append({\"array\": waveform.numpy(), \"sampling_rate\": 16000})\n",
    "\n",
    "#         # Match .txt transcription file by name\n",
    "#         txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "#         txt_path = os.path.join(txt_folder, txt_filename)\n",
    "#         if os.path.isfile(txt_path):\n",
    "#             print(f\"Processing transcription file {txt_path}\")\n",
    "#             with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 transcription = f.read().strip()\n",
    "#                 data[\"text\"].append(transcription)\n",
    "                \n",
    "#                 # Tokenize the transcription text with padding and truncation\n",
    "#                 max_word_len = 357\n",
    "#                 tokens = tokenizer(transcription, return_tensors=\"pt\" ,padding = 'max_length', max_length = max_word_len, truncation=True)\n",
    "#                 data[\"input_ids\"].append(tokens.input_ids[0].numpy())  # Convert to numpy array for compatibility\n",
    "#                 data[\"attention_mask\"].append(tokens.attention_mask[0].numpy())  # Store attention mask for padding\n",
    "#         else:\n",
    "#             print(f\"Warning: No matching transcription found for {filename}\")\n",
    "#             data[\"text\"].append(\"\")\n",
    "\n",
    "# # Convert data into Hugging Face Dataset object\n",
    "# dataset = Dataset.from_dict(data)\n",
    "\n",
    "# # Preprocess the audio data\n",
    "\n",
    "# # Apply preprocessing to audio data\n",
    "\n",
    "# # Now, `dataset` contains padded audio, transcriptions, and tokenized text ready for fine-tuning\n",
    "# print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(\"/storage/student5/xuan_quy/asrdata/dataset\")\n",
    "# print(dataset['input_ids'][0])\n",
    "# print(dataset['attention_mask'][0])\n",
    "# print(dataset['text'][0])\n",
    "# # print(dataset['audio'][0])\n",
    "# print(dataset['audio'][0]['array'])\n",
    "# print(dataset['audio'][0]['sampling_rate'])\n",
    "# from datasets import load_from_disk\n",
    "\n",
    "# dataset = load_from_disk(\"/storage/student5/xuan_quy/asrdata/dataset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# max_word_len = 357\n",
    "# def preprocess_function(examples):\n",
    "#     inputs = processor(\n",
    "#         examples[\"audio\"][\"array\"],\n",
    "#         sampling_rate=examples[\"audio\"][\"sampling_rate\"],\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True\n",
    "#     )\n",
    "#     input_values = inputs.input_values.squeeze()\n",
    "    \n",
    "#     with processor.as_target_processor():\n",
    "#         labels = processor.tokenizer(examples[\"text\"],return_tensors=\"pt\" ,padding = 'max_length', max_length = max_word_len, truncation=True).input_ids\n",
    "    \n",
    "#     return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# dataset = dataset.map(preprocess_function)\n",
    "# # split dataset into train, validation and test set \n",
    "# train_dataset = dataset.train_test_split(test_size=0.1, shuffle=True)\n",
    "# train_dat= train_dataset['train']\n",
    "# val_dat = train_dataset['test']\n",
    "# print(train_dat)\n",
    "# print(val_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #save dataset \n",
    "# train_dat.save_to_disk('/storage/student5/xuan_quy/asrdata/train')\n",
    "# val_dat.save_to_disk('/storage/student5/xuan_quy/asrdata/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" \n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./wav2vec2_finetune\",       # Directory to save checkpoints\n",
    "#     per_device_train_batch_size=8,          # Adjust based on available GPU memory\n",
    "#     per_device_eval_batch_size=8,           # Same as train batch size for simplicity\n",
    "#     evaluation_strategy=\"no\",            # Evaluate at certain steps\n",
    "#     logging_steps=50,                       # Log every 50 steps\n",
    "#     save_steps=400,                         # Save every 400 steps\n",
    "#     eval_steps=400,                         # Evaluate every 400 steps\n",
    "#     learning_rate=3e-4,                     # Adjust as needed for your data\n",
    "#     num_train_epochs=10,                    # Set based on dataset size\n",
    "#     warmup_steps=500,                       # Warmup to prevent sudden gradient changes\n",
    "#     save_total_limit=2,                     # Keep only the 2 latest checkpoints\n",
    "#     fp16=True,                              # Use mixed precision for faster training if supported\n",
    "#     gradient_accumulation_steps=2,        \n",
    "#       # Adjust if using small batches\n",
    "# )\n",
    "\n",
    "# # class DataCollatorCTCWithPadding:\n",
    "# #     def __init__(self, processor, padding=\"longest\"):\n",
    "# #         self.processor = processor\n",
    "# #         self.padding = padding\n",
    "\n",
    "# #     def __call__(self, features):\n",
    "# #         # Separate audio and label features\n",
    "# #         audio_features = [feature[\"input_values\"] for feature in features]\n",
    "# #         label_features = [feature[\"labels\"] for feature in features if \"labels\" in feature]\n",
    "\n",
    "# #         # Pad input values dynamically to the longest sequence in the batch\n",
    "# #         batch = self.processor(\n",
    "# #             audio_features, \n",
    "# #             padding=self.padding,  # Dynamic padding per batch\n",
    "# #             return_tensors=\"pt\"\n",
    "# #         )\n",
    "\n",
    "# #         # Pad labels dynamically if available\n",
    "# #         if label_features:\n",
    "# #             labels = torch.nn.utils.rnn.pad_sequence(\n",
    "# #                 [torch.tensor(label, dtype=torch.long) for label in label_features],\n",
    "# #                 batch_first=True,\n",
    "# #                 padding_value=self.processor.tokenizer.pad_token_id\n",
    "# #             )\n",
    "# #             batch[\"labels\"] = labels\n",
    "\n",
    "# #         return batch\n",
    "\n",
    "# # # from transformers import DataCollatorCTCWithPadding\n",
    "\n",
    "\n",
    "\n",
    "# # # Then initialize it like this:\n",
    "# # data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")\n",
    "\n",
    "\n",
    "# #Define evaluation metric\n",
    "# import evaluate\n",
    "\n",
    "# # Load the Word Error Rate (WER) metric\n",
    "# wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "# # Define compute_metrics function\n",
    "# def compute_metrics(pred):\n",
    "#     pred_logits = pred.predictions\n",
    "#     pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "#     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "#     pred_str = processor.batch_decode(pred_ids)\n",
    "#     # we do not want to group tokens when computing the metrics\n",
    "#     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "#     wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "#     return {\"wer\": wer}\n",
    "\n",
    "\n",
    "# # Initialize Trainer\n",
    "# from transformers import Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dat,\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the model\n",
    "# model.save_pretrained(\"./wav2vec2_finetuned\")\n",
    "# processor.save_pretrained(\"./wav2vec2_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mfccs = []\n",
    "# audio_dir = \"/storage/student5/xuan_quy/asrdata/wav/\"\n",
    "# max_len= 2521\n",
    "# n_mfcc =20\n",
    "# sample_rate = 16000\n",
    "# print(\"Starting\")\n",
    "# for i in range(50000, len(os.listdir(audio_dir))):\n",
    "# # for i in range(10):\n",
    "#     y, sr = librosa.load(os.path.join(audio_dir, f\"{i}.wav\"), sr=sample_rate)\n",
    "    \n",
    "#     # Compute the MFCC features\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "#     # Transpose the MFCC to match the model's expected input shape\n",
    "#     mfcc = mfcc.T  # Shape becomes (time_steps, n_mfcc)\n",
    "    \n",
    "#     # Padding/truncating to `max_len`\n",
    "#     if mfcc.shape[0] < max_len:\n",
    "#         # Pad with zeros if shorter\n",
    "#         pad_width = max_len - mfcc.shape[0]\n",
    "#         mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')\n",
    "#     else:\n",
    "#         # Truncate if longer\n",
    "#         mfcc = mfcc[:max_len, :]\n",
    "#     # audio, sr = librosa.load(os.path.join(audio_dir, f\"{i}.wav\"), sr=16000)\n",
    "#     # # audio = np.vstack([audio, audio])\n",
    "#     # mfcc = librosa.feature.mfcc(y=audio, sr=sr)\n",
    "#     # if mfcc.shape[1] < max_len:\n",
    "#     #     # Pad with zeros if shorter\n",
    "#     #     pad_width = max_len - mfcc.shape[1]\n",
    "#     #     mfcc = np.pad(mfcc, (0, pad_width), mode='constant')\n",
    "#     # else:\n",
    "#     #     # Truncate if longer\n",
    "#     #     mfcc = mfcc[:max_len, :]\n",
    "#     print(f\"Converted {i} audio\")\n",
    "#     mfccs.append(mfcc)\n",
    "#     del(mfcc)\n",
    "#     gc.collect()\n",
    "# print(\"Saving npy\")\n",
    "# np.save('x5.npy', mfccs)\n",
    "# print(\"Done :>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get subword in txt folder:\n",
    "# import os\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Define the path to the text files\n",
    "# txt_folder = \"/storage/student5/xuan_quy/asrdata/txt\"\n",
    "\n",
    "# # Initialize an empty set to store unique subwords\n",
    "# subwords = set()\n",
    "\n",
    "# # Loop through all .txt files in the folder\n",
    "# for filename in os.listdir(txt_folder):\n",
    "#     if filename.endswith(\".txt\"):\n",
    "#         with open(os.path.join(txt_folder, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "#             # Read the text file and convert to lowercase\n",
    "#             text = f.read().lower()\n",
    "#             # Tokenize the text into subwords\n",
    "#             words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "#             subwords.update(words)\n",
    "\n",
    "# # Convert the set of subwords to a list\n",
    "# subwords = list(subwords)\n",
    "# print(f\"Number of unique subwords: {len(subwords)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "# # Tokenize the subwords\n",
    "# with open ('/storage/student5/xuan_quy/asrdata/txt/0.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read().lower()\n",
    "#     print(f\"text: {text}\")\n",
    "#     # words = re.findall(r\"\\b\\w+\\b\", text)\n",
    "#     # print(f\"word: {words}\")\n",
    "#     # subwords = \" \".join(words)\n",
    "#     # print(f\"subwords: {subwords}\")\n",
    "#     encoded_inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#     print(encoded_inputs)\n",
    "#     print(encoded_inputs.input_ids.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir /storage/student5/xuan_quy/asrdata/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import re \n",
    "# import os\n",
    "# import gc\n",
    "# # Initialize the tokenizer (for subword tokenization)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")  # Replace with chosen tokenizer model\n",
    "\n",
    "# # Directory containing the text files\n",
    "# transcript_dir = \"/storage/student5/xuan_quy/asrdata/txt\"\n",
    "\n",
    "# # Use a set to store unique tokens\n",
    "# unique_tokens = set()\n",
    "# labels = []\n",
    "# print(len(os.listdir(transcript_dir)))\n",
    "# for i in range(len(os.listdir(transcript_dir))):\n",
    "# # # Loop through each file in the directory\n",
    "# # for filename in os.listdir(transcript_dir):\n",
    "# #     if filename.endswith(\".txt\"):\n",
    "#     with open(os.path.join(transcript_dir, f\"{i}.txt\"), 'r') as file:\n",
    "#         text = file.read().lower()\n",
    "        \n",
    "#         max_length = 357# or 512 depending on your specific model configuration\n",
    "#         # Tokenize the text\n",
    "#         token = tokenizer(text, return_tensors=\"pt\", padding='max_length', max_length=max_length, truncation=True)\n",
    "#         print(token['input_ids'].shape)\n",
    "#         labels.append(token['input_ids'].squeeze().numpy())\n",
    "#         print(f\"Processed file {i}.txt\")\n",
    "\n",
    "\n",
    "# # Save the labels to a numpy file\n",
    "#         # labels.append(tokens)\n",
    "# # The total number of unique IDs\n",
    "# np.save('y.npy', np.array(labels))\n",
    "# # print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x =torch.load('padded_audio.pt')\n",
    "# y = torch.load('padded_encoded_texts.pt')\n",
    "# # print(x[0])\n",
    "# print(y[0])\n",
    "# print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x_train = x[:int(x.shape[0]*0.7),: , :]\n",
    "# y_train = y[:int(y.shape[0]*0.7), :]\n",
    "\n",
    "# # x_val = x[:int(x.shape[0]*0.2),: ,: ]\n",
    "# y_val = y[:int(y.shape[0]*0.2) ,: ]\n",
    "\n",
    "# # x_test = x[:int(x.shape[0]*0.1), :, :]\n",
    "# y_test = y[:int(y.shape[0]*0.1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# # np.save('x_train.npy', x_train)\n",
    "# np.save('y_train.npy', y_train)\n",
    "\n",
    "# # np.save('x_val.npy', x_val)\n",
    "# np.save('y_val.npy', y_val)\n",
    "\n",
    "# # np.save('x_test.npy', x_test)\n",
    "# np.save('y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.transforms import MFCC\n",
    "import numpy as np\n",
    "\n",
    "# QuartzNet Block Definition\n",
    "class QuartzNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, repeat=10, dropout=0.2):\n",
    "        super(QuartzNetBlock, self).__init__()\n",
    "        self.layers = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels if i == 0 else out_channels, out_channels, kernel_size, padding=kernel_size // 2),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            for i in range(repeat)\n",
    "        ])\n",
    "        self.residual = nn.Conv1d(in_channels, out_channels, 1)  # Residual connection\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x) + self.residual(x)\n",
    "\n",
    "# QuartzNet Model Definition\n",
    "class QuartzNet(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, dropout=0.2):\n",
    "        super(QuartzNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            QuartzNetBlock(num_features, 64, kernel_size=33, dropout=dropout),\n",
    "            QuartzNetBlock(64, 128, kernel_size=39, dropout=dropout),\n",
    "            QuartzNetBlock(128, 256, kernel_size=41, dropout=dropout),\n",
    "            QuartzNetBlock(256, 512, kernel_size=41, dropout=dropout),\n",
    "            QuartzNetBlock(512, 512, kernel_size=41, dropout=dropout),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv1d(512, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        # x = x.permute(2, 0, 1)  # Prepare for CTC loss (T, N, C)\n",
    "        return x    \n",
    "\n",
    "# Custom Dataset for Speech Data\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "\n",
    "# Data Collator for Padding\n",
    "def collate_fn(batch):\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    x_batch = [torch.tensor(x).permute(1,0) for x in x_batch]\n",
    "    y_batch = [torch.tensor(y) for y in y_batch]\n",
    "\n",
    "    # Pad MFCC features to the length of the longest sequence in the batch\n",
    "    x_batch = nn.utils.rnn.pad_sequence(x_batch, batch_first=True)  # (N, C, T)\n",
    "\n",
    "    # Calculate input lengths after padding\n",
    "    input_lengths = torch.tensor([x.size(-1) for x in x_batch])  # Lengths of each input (time dimension)\n",
    "\n",
    "    # Pad labels to the length of the longest sequence in the batch\n",
    "    y_batch = nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value=-100)  # -100 is ignored by CTC loss\n",
    "    target_lengths = torch.tensor([len(y) for y in y_batch])  # Lengths of each target\n",
    "\n",
    "    return x_batch, y_batch, input_lengths, target_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39498, 2521, 64) (39498, 349)\n",
      "Epoch 1, Loss: 1.0101019144058228\n",
      "Validation Loss: 0.7999359704870678\n",
      "Epoch 2, Loss: 0.5185635685920715\n",
      "Validation Loss: 0.8297744600192007\n",
      "Epoch 3, Loss: 0.687262237071991\n",
      "Validation Loss: 0.6365209237036556\n",
      "Epoch 4, Loss: 0.45351049304008484\n",
      "Validation Loss: 0.6529383510351181\n",
      "Epoch 5, Loss: 0.6791920065879822\n",
      "Validation Loss: 0.6377184350497325\n",
      "Epoch 6, Loss: 0.6872736811637878\n",
      "Validation Loss: 0.6156616050950529\n",
      "Epoch 7, Loss: 0.6830143332481384\n",
      "Validation Loss: 0.7228416869306024\n",
      "Epoch 8, Loss: 0.6053295135498047\n",
      "Validation Loss: 0.6159043059639485\n",
      "Epoch 9, Loss: 0.6306182146072388\n",
      "Validation Loss: 0.6126926940240536\n",
      "Epoch 10, Loss: 0.6308901906013489\n",
      "Validation Loss: 0.6373259954513302\n",
      "Epoch 11, Loss: 0.49910327792167664\n",
      "Validation Loss: 0.5814371306987389\n",
      "Epoch 12, Loss: 3.187992811203003\n",
      "Validation Loss: 0.6833472720877645\n",
      "Epoch 13, Loss: 0.5284697413444519\n",
      "Validation Loss: 0.5652847995416976\n",
      "Epoch 14, Loss: 0.4979622960090637\n",
      "Validation Loss: 0.5776257582494287\n",
      "Epoch 15, Loss: 0.5056623816490173\n",
      "Validation Loss: 0.5706219406793205\n",
      "Epoch 16, Loss: 0.6670267581939697\n",
      "Validation Loss: 0.5737872582994845\n",
      "Epoch 17, Loss: 0.46222123503685\n",
      "Validation Loss: 0.576733337498927\n",
      "Epoch 18, Loss: 0.6060580015182495\n",
      "Validation Loss: 0.5883985406616254\n",
      "Epoch 19, Loss: 0.6207151412963867\n",
      "Validation Loss: 0.6593894220192101\n",
      "Epoch 20, Loss: 0.4798266589641571\n",
      "Validation Loss: 0.5743600226748766\n",
      "Epoch 21, Loss: 1.098232626914978\n",
      "Validation Loss: 0.7122016657597621\n",
      "Epoch 22, Loss: 0.5115424990653992\n",
      "Validation Loss: 0.5711301191585936\n",
      "Epoch 23, Loss: 1.5156668424606323\n",
      "Validation Loss: 0.6148184146573793\n",
      "Epoch 24, Loss: 0.5217276811599731\n",
      "Validation Loss: 0.580726737847747\n",
      "Epoch 25, Loss: 0.7836464047431946\n",
      "Validation Loss: 0.6128752362880086\n",
      "Epoch 26, Loss: 0.5242516398429871\n",
      "Validation Loss: 0.5976003328227456\n",
      "Epoch 27, Loss: 0.5962111353874207\n",
      "Validation Loss: 0.5736990464645472\n",
      "Epoch 28, Loss: 0.4388020634651184\n",
      "Validation Loss: 0.5611909350997666\n",
      "Epoch 29, Loss: 0.4882340431213379\n",
      "Validation Loss: 0.5855995305621252\n",
      "Epoch 30, Loss: 0.515408456325531\n",
      "Validation Loss: 0.5996668358193579\n",
      "Epoch 31, Loss: 0.566830575466156\n",
      "Validation Loss: 0.5678118188303524\n",
      "Epoch 32, Loss: 0.5578412413597107\n",
      "Validation Loss: 0.5654803394595894\n",
      "Epoch 33, Loss: 0.6866442561149597\n",
      "Validation Loss: 0.6516888051718558\n",
      "Epoch 34, Loss: 0.5885433554649353\n",
      "Validation Loss: 0.5684588409685886\n",
      "Epoch 35, Loss: 0.3883034884929657\n",
      "Validation Loss: 0.5671536170220578\n",
      "Epoch 36, Loss: 0.7187948226928711\n",
      "Validation Loss: 0.6692455871078178\n",
      "Epoch 37, Loss: 0.545432984828949\n",
      "Validation Loss: 0.584280511306298\n",
      "Epoch 38, Loss: 0.5318279266357422\n",
      "Validation Loss: 0.5810315886317164\n",
      "Epoch 39, Loss: 0.6425869464874268\n",
      "Validation Loss: 0.5619778940849871\n",
      "Epoch 40, Loss: 0.4652771055698395\n",
      "Validation Loss: 0.5645000856084796\n",
      "Epoch 41, Loss: 0.6063356995582581\n",
      "Validation Loss: 0.5608298579711076\n",
      "Epoch 42, Loss: 0.6910916566848755\n",
      "Validation Loss: 0.5589419588573256\n",
      "Epoch 43, Loss: 0.785373330116272\n",
      "Validation Loss: 0.5592492783373544\n",
      "Epoch 44, Loss: 0.5090527534484863\n",
      "Validation Loss: 0.5701789611544217\n",
      "Epoch 45, Loss: 0.9426823854446411\n",
      "Validation Loss: 0.5795619192272002\n",
      "Epoch 46, Loss: 0.4783652722835541\n",
      "Validation Loss: 0.5663372508273922\n",
      "Epoch 47, Loss: 0.5058057904243469\n",
      "Validation Loss: 0.5706502393352412\n",
      "Epoch 48, Loss: 0.5846232175827026\n",
      "Validation Loss: 0.5817617373831211\n",
      "Epoch 49, Loss: 0.3473069965839386\n",
      "Validation Loss: 0.5614194505908672\n",
      "Epoch 50, Loss: 0.5708444118499756\n",
      "Validation Loss: 0.5563397293767915\n",
      "Epoch 51, Loss: 0.4037727415561676\n",
      "Validation Loss: 0.5671780395186994\n",
      "Epoch 52, Loss: 0.7275922298431396\n",
      "Validation Loss: 0.5795126347974069\n",
      "Epoch 53, Loss: 0.4773256778717041\n",
      "Validation Loss: 0.5673846877245997\n",
      "Epoch 54, Loss: 0.5405715107917786\n",
      "Validation Loss: 0.5682181366442283\n",
      "Epoch 55, Loss: 0.5362225770950317\n",
      "Validation Loss: 0.5607294855644616\n",
      "Epoch 56, Loss: 0.5954076051712036\n",
      "Validation Loss: 0.5844875840897601\n",
      "Epoch 57, Loss: 0.46151718497276306\n",
      "Validation Loss: 0.5702112434437188\n",
      "Epoch 58, Loss: 0.5675191283226013\n",
      "Validation Loss: 0.5740714482224359\n",
      "Epoch 59, Loss: 0.49963217973709106\n",
      "Validation Loss: 0.5624759472885483\n",
      "Epoch 60, Loss: 0.5727441906929016\n",
      "Validation Loss: 0.562379089867427\n",
      "Epoch 61, Loss: 0.48981237411499023\n",
      "Validation Loss: 0.5582816995068245\n",
      "Epoch 62, Loss: 0.3281007707118988\n",
      "Validation Loss: 0.5636190342751171\n",
      "Epoch 63, Loss: 0.6228177547454834\n",
      "Validation Loss: 0.5614186959293679\n",
      "Epoch 64, Loss: 0.6437321901321411\n",
      "Validation Loss: 0.5866113352792459\n",
      "Epoch 65, Loss: 0.41073575615882874\n",
      "Validation Loss: 0.5837012356300192\n",
      "Epoch 66, Loss: 0.5158754587173462\n",
      "Validation Loss: 0.5637362963248245\n",
      "Epoch 67, Loss: 0.6137375235557556\n",
      "Validation Loss: 0.5883558519585612\n",
      "Epoch 68, Loss: 0.47600188851356506\n",
      "Validation Loss: 0.5852543676203439\n",
      "Epoch 69, Loss: 0.656261146068573\n",
      "Validation Loss: 0.562620384412852\n",
      "Epoch 70, Loss: 0.5724482536315918\n",
      "Validation Loss: 0.5762870470879774\n",
      "Epoch 71, Loss: 0.4226440489292145\n",
      "Validation Loss: 0.5802382977927373\n",
      "Epoch 72, Loss: 0.5330976247787476\n",
      "Validation Loss: 0.575437985339516\n",
      "Epoch 73, Loss: 0.4024229049682617\n",
      "Validation Loss: 0.627485065048207\n",
      "Epoch 74, Loss: 0.5829071402549744\n",
      "Validation Loss: 0.5698729830321779\n",
      "Epoch 75, Loss: 0.6331188678741455\n",
      "Validation Loss: 0.5817614007350743\n",
      "Epoch 76, Loss: 0.5063723921775818\n",
      "Validation Loss: 0.5917785957075719\n",
      "Epoch 77, Loss: 0.524945855140686\n",
      "Validation Loss: 0.5851615033727192\n",
      "Epoch 78, Loss: 0.5738375186920166\n",
      "Validation Loss: 0.5681210709867667\n",
      "Epoch 79, Loss: 0.5885626673698425\n",
      "Validation Loss: 0.6143142209934446\n",
      "Epoch 80, Loss: 0.47620660066604614\n",
      "Validation Loss: 0.5779552675002695\n",
      "Epoch 81, Loss: 0.5007814168930054\n",
      "Validation Loss: 0.5818192228126121\n",
      "Epoch 82, Loss: 0.647859513759613\n",
      "Validation Loss: 0.5880953193808413\n",
      "Epoch 83, Loss: 0.5157744288444519\n",
      "Validation Loss: 0.578958633601159\n",
      "Epoch 84, Loss: 0.35314711928367615\n",
      "Validation Loss: 0.5840447856160804\n",
      "Epoch 85, Loss: 0.4894472062587738\n",
      "Validation Loss: 0.5710938713452633\n",
      "Epoch 86, Loss: 0.5385972261428833\n",
      "Validation Loss: 0.5679119903034934\n",
      "Epoch 87, Loss: 0.5965127944946289\n",
      "Validation Loss: 0.5798390452399133\n",
      "Epoch 88, Loss: 0.5110227465629578\n",
      "Validation Loss: 0.5809741294164495\n",
      "Epoch 89, Loss: 0.5138506293296814\n",
      "Validation Loss: 0.5874089737958341\n",
      "Epoch 90, Loss: 0.5512653589248657\n",
      "Validation Loss: 0.5779033104015815\n",
      "Epoch 91, Loss: 0.5173619389533997\n",
      "Validation Loss: 0.5790746704630406\n",
      "Epoch 92, Loss: 0.5596598982810974\n",
      "Validation Loss: 0.5692824733578112\n",
      "Epoch 93, Loss: 0.5510140657424927\n",
      "Validation Loss: 0.5709599091462981\n",
      "Epoch 94, Loss: 0.49126601219177246\n",
      "Validation Loss: 0.5679154562798168\n",
      "Epoch 95, Loss: 0.4868917465209961\n",
      "Validation Loss: 0.5888630677721656\n",
      "Epoch 96, Loss: 0.5671273469924927\n",
      "Validation Loss: 0.5857910032228437\n",
      "Epoch 97, Loss: 0.6109322309494019\n",
      "Validation Loss: 0.5896353331993389\n",
      "Epoch 98, Loss: 0.4564323425292969\n",
      "Validation Loss: 0.5819926589896254\n",
      "Epoch 99, Loss: 0.6013891100883484\n",
      "Validation Loss: 0.5723565997550575\n",
      "Epoch 100, Loss: 0.5595675706863403\n",
      "Validation Loss: 0.5992113732582449\n",
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Dataset and DataLoader\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy') \n",
    "x_val = np.load('x_val.npy')\n",
    "y_val = np.load('y_val.npy')\n",
    "\n",
    "# print(f\"training shape: {x_train.shape}, training target shape: {y_train.shape}\")\n",
    "# print(f\"first batch of x valdition: {x_val[0]}\")\n",
    "# print(f\"first batch of y valdition:{y_val[0]}\")\n",
    "\n",
    "print(x_train.shape, y_train.shape) # Load your precomputed MFCC features and tokenized labels here\n",
    "train_dataset = SpeechDataset(x_train, y_train)\n",
    "val_dataset = SpeechDataset(x_val, y_val)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "device = 'cuda:3'\n",
    "epoch = 100\n",
    "\n",
    "\n",
    "def evaluate(model,device, dataloader, ctc_loss):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, input_lengths, target_lengths in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            x_batch = x_batch.permute(0, 2, 1)  # (N, C, T) -> (N, T, C)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x_batch)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            log_probs = log_probs.permute(1, 0, 2)  # (T, N, C) -> (N, T, C)\n",
    "\n",
    "            # Calculate CTC loss\n",
    "            loss = ctc_loss(log_probs, y_batch, input_lengths, target_lengths)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train(model, device, train_dataloader, val_dataloader, optimizer, ctc_loss, num_epochs):\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    model.to(device)\n",
    "    #training loop\n",
    "    for epoch in range(num_epochs):  # Number of epochs        \n",
    "        model.train()\n",
    "        for x_batch, y_batch, input_lengths, target_lengths in train_dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # print(x_batch.shape)\n",
    "            x_batch = x_batch.permute(0, 2, 1)  # (N, C, T) -> (N, T, C)\n",
    "            # Forward pass\n",
    "            logits = model(x_batch)  # (N, C, T) -> (T, N, C)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            \n",
    "            # print(log_probs.shape)\n",
    "            # print(y_batch.shape)\n",
    "            log_probs = log_probs.permute(1, 0, 2)  # (T, N, C) -> (N, T, C)\n",
    "            # Calculate CTC loss\n",
    "            loss = ctc_loss(log_probs, y_batch, input_lengths, target_lengths)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()     \n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "        train_losses.append(loss.item())\n",
    "        #add checkpoint:\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_path = f\"quartznet_checkpoint_{epoch + 1}.pt\"\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # Evaluate the model\n",
    "        val_loss = evaluate(model, device, val_dataloader, ctc_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    save_path = f\"quartznet_{num_epochs}_2.pt\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "# Initialize Model, Optimizer, and Loss Function\n",
    "num_features = x_train[0].shape[0]  # Number of MFCC features per frame\n",
    "num_classes = 8000  # Vocabulary size (number of unique tokens in y_train)\n",
    "\n",
    "model = QuartzNet(num_features=num_features, num_classes=num_classes)\n",
    "# if os.path.exists('quartznet_checkpoint_30.pt'):\n",
    "#     model.load_state_dict(torch.load('quartznet_checkpoint_10.pt'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "ctc_loss = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "\n",
    "train_losses, val_losses = train(model, device,train_dataloader, val_dataloader, optimizer, ctc_loss, epoch)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QuartzNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mQuartzNet\u001b[49m(num_features\u001b[38;5;241m=\u001b[39mnum_features, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquartznet_100_2.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m2521\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'QuartzNet' is not defined"
     ]
    }
   ],
   "source": [
    "model = QuartzNet(num_features=num_features, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load('quartznet_100_2.pt'))\n",
    "input = torch.randn(16,64,2521).permute(0,2,1)\n",
    "input_names = 'audio'\n",
    "output_names = 'text'\n",
    "output = model(input)\n",
    "torch.onnx.export(model, input, 'quartznet.onnx', input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class BeamSearchDecoder:\n",
    "    def __init__(self, tokenizer, beam_width=5, blank_token_id=0):\n",
    "        \"\"\"\n",
    "        Initialize Beam Search Decoder for CTC decoding\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: BPE Tokenizer with decode method\n",
    "            beam_width: Number of paths to explore simultaneously\n",
    "            blank_token_id: ID of the blank token in CTC\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.beam_width = beam_width\n",
    "        self.blank_token_id = blank_token_id\n",
    "\n",
    "    def decode(self, log_probs: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        Perform beam search decoding on CTC output\n",
    "        \n",
    "        Args:\n",
    "            log_probs: Log probabilities of shape (time_steps, num_classes)\n",
    "        \n",
    "        Returns:\n",
    "            Decoded text string\n",
    "        \"\"\"\n",
    "        # Convert log probabilities to numpy for easier manipulation\n",
    "        log_probs = log_probs.cpu().numpy()\n",
    "        \n",
    "        # Initialize beam\n",
    "        beam = [({'sequence': [], 'score': 0.0, 'last_token': None}, )]\n",
    "        \n",
    "        # Iterate through time steps\n",
    "        for t in range(log_probs.shape[0]):\n",
    "            next_beam = []\n",
    "            \n",
    "            for candidate, _ in beam:\n",
    "                # Current beam state\n",
    "                sequence = candidate['sequence']\n",
    "                score = candidate['score']\n",
    "                last_token = candidate['last_token']\n",
    "                \n",
    "                # Top k probabilities for current time step\n",
    "                top_k_indices = np.argsort(log_probs[t])[::-1][:self.beam_width]\n",
    "                \n",
    "                for token_id in top_k_indices:\n",
    "                    token_log_prob = log_probs[t][token_id]\n",
    "                    \n",
    "                    # Handle blank token\n",
    "                    if token_id == self.blank_token_id:\n",
    "                        # Blank token doesn't change sequence\n",
    "                        new_candidate = {\n",
    "                            'sequence': sequence.copy(),\n",
    "                            'score': score + token_log_prob,\n",
    "                            'last_token': None\n",
    "                        }\n",
    "                        next_beam.append((new_candidate, ))\n",
    "                        continue\n",
    "                    \n",
    "                    # Extend sequence logic\n",
    "                    new_sequence = sequence.copy()\n",
    "                    \n",
    "                    # Prevent repeated tokens unless they're different\n",
    "                    if last_token != token_id:\n",
    "                        new_sequence.append(token_id)\n",
    "                    \n",
    "                    new_candidate = {\n",
    "                        'sequence': new_sequence,\n",
    "                        'score': score + token_log_prob,\n",
    "                        'last_token': token_id\n",
    "                    }\n",
    "                    next_beam.append((new_candidate, ))\n",
    "            \n",
    "            # Prune beam to top k candidates\n",
    "            next_beam.sort(key=lambda x: x[0]['score'], reverse=True)\n",
    "            beam = next_beam[:self.beam_width]\n",
    "        \n",
    "        # Select best sequence\n",
    "        best_candidate = max(beam, key=lambda x: x[0]['score'])[0]\n",
    "        \n",
    "        # Decode sequence back to text\n",
    "        decoded_text = self.tokenizer.decode(best_candidate['sequence'])\n",
    "        \n",
    "        return decoded_text\n",
    "\n",
    "def test_speech_recognition_model(\n",
    "    model: torch.nn.Module, \n",
    "    test_dataloader: torch.utils.data.DataLoader, \n",
    "    device: str, \n",
    "    tokenizer: Any, \n",
    "    beam_search_decoder: BeamSearchDecoder\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Test the speech recognition model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained QuartzNet model\n",
    "        test_dataloader: DataLoader for test dataset\n",
    "        device: Device to run inference on\n",
    "        tokenizer: Tokenizer used for decoding\n",
    "        beam_search_decoder: Beam search decoder instance\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of performance metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Metrics to track\n",
    "    total_samples = 0\n",
    "    correct_predictions = 0\n",
    "    total_wer = 0.0  # Word Error Rate\n",
    "    total_cer = 0.0  # Character Error Rate\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, input_lengths, target_lengths in test_dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Prepare input\n",
    "            x_batch = x_batch.permute(0, 2, 1)  # (N, C, T) -> (N, T, C)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x_batch)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            log_probs = log_probs.permute(1, 0, 2)  # (T, N, C)\n",
    "            \n",
    "            # Process each sample in batch\n",
    "            for i in range(x_batch.size(0)):\n",
    "                # Ground truth\n",
    "                true_tokens = y_batch[i][:target_lengths[i]].cpu().numpy()\n",
    "                true_text = tokenizer.decode(true_tokens)\n",
    "                \n",
    "                # Prediction using beam search\n",
    "                sample_log_probs = log_probs[:, i, :]\n",
    "                pred_text = beam_search_decoder.decode(sample_log_probs)\n",
    "                \n",
    "                # Calculate error rates\n",
    "                wer = word_error_rate(true_text.split(), pred_text.split())\n",
    "                cer = character_error_rate(true_text, pred_text)\n",
    "                \n",
    "                total_wer += wer\n",
    "                total_cer += cer\n",
    "                total_samples += 1\n",
    "                \n",
    "                # Simple accuracy (might not be the best metric for ASR)\n",
    "                if pred_text == true_text:\n",
    "                    correct_predictions += 1\n",
    "        \n",
    "        # Compute final metrics\n",
    "        metrics = {\n",
    "            'accuracy': correct_predictions / total_samples,\n",
    "            'word_error_rate': total_wer / total_samples,\n",
    "            'character_error_rate': total_cer / total_samples\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def word_error_rate(reference: List[str], hypothesis: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Compute Word Error Rate using Levenshtein distance\n",
    "    \n",
    "    Args:\n",
    "        reference: List of ground truth words\n",
    "        hypothesis: List of predicted words\n",
    "    \n",
    "    Returns:\n",
    "        Word Error Rate\n",
    "    \"\"\"\n",
    "    # Levenshtein distance implementation\n",
    "    d = np.zeros((len(reference) + 1, len(hypothesis) + 1))\n",
    "    \n",
    "    for i in range(len(reference) + 1):\n",
    "        for j in range(len(hypothesis) + 1):\n",
    "            if i == 0:\n",
    "                d[i][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][j] = i\n",
    "            elif reference[i-1] == hypothesis[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion = d[i][j-1] + 1\n",
    "                deletion = d[i-1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    \n",
    "    return d[len(reference)][len(hypothesis)] / len(reference)\n",
    "\n",
    "def character_error_rate(reference: str, hypothesis: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute Character Error Rate using Levenshtein distance\n",
    "    \n",
    "    Args:\n",
    "        reference: Ground truth text\n",
    "        hypothesis: Predicted text\n",
    "    \n",
    "    Returns:\n",
    "        Character Error Rate\n",
    "    \"\"\"\n",
    "    d = np.zeros((len(reference) + 1, len(hypothesis) + 1))\n",
    "    \n",
    "    for i in range(len(reference) + 1):\n",
    "        for j in range(len(hypothesis) + 1):\n",
    "            if i == 0:\n",
    "                d[i][j] = j\n",
    "            elif j == 0:\n",
    "                d[i][j] = i\n",
    "            elif reference[i-1] == hypothesis[j-1]:\n",
    "                d[i][j] = d[i-1][j-1]\n",
    "            else:\n",
    "                substitution = d[i-1][j-1] + 1\n",
    "                insertion = d[i][j-1] + 1\n",
    "                deletion = d[i-1][j] + 1\n",
    "                d[i][j] = min(substitution, insertion, deletion)\n",
    "    \n",
    "    return d[len(reference)][len(hypothesis)] / len(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_batch, y_batch, input_lengths, target_lengths = next(iter(test_loader))\n",
    "        x_batch = x_batch.to(device)\n",
    "\n",
    "        x_batch = x_batch.permute(0, 2, 1)  # (N, C, T) -> (N, T, C)\n",
    "        output = model(x_batch)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "\n",
    "        # Use greedy decoding\n",
    "        decoded_preds = torch.argmax(output, dim=-1)\n",
    "        print(\"True:\", y_batch[0])\n",
    "        print(\"Predicted:\", decoded_preds[0])\n",
    "        # print(\"WER:\", jiwer.wer(y_batch, decoded_preds))\n",
    "\n",
    "def word_error_rate(preds, targets):\n",
    "    wer = 0\n",
    "    for pred, target in zip(preds, targets):\n",
    "        wer += jiwer.wer(target, pred)\n",
    "    return wer / len(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_328469/3155948349.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"quartznet_100_2.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: tensor([1582, 3607, 1582, 1987,  135,  805,  124,  509,  135,  184, 1342, 1582,\n",
      "        1147,  507,  186,  437,  176,  513, 1361,  477,  859,  263,  245, 1026,\n",
      "         168,  124,  184,  166,  186,  161,  477,  859,  713,  263,  213,  307,\n",
      "         587,  294,  184,  199,  154, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001, 8001,\n",
      "        8001])\n",
      "Predicted: tensor([ 0,  0,  0,  ..., 16,  1, 29], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "#beam search algorithm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#parameters\n",
    "# x_train = np.load('x_train.npy')\n",
    "num_features = 2521\n",
    "print (num_features)  # Number of MFCC features per frame\n",
    "num_classes = 8000  # Vocabulary size (number of unique tokens in y_train)\n",
    "device = 'cuda:3'\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = QuartzNet(num_features=num_features, num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(\"quartznet_100_2.pt\"))\n",
    "model.to(device)\n",
    "\n",
    "#Load the test dataset:\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "test_dataset = SpeechDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "# Test the model\n",
    "test(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Optional, Tuple\n",
    "# class ASRConfig:\n",
    "#     \"\"\"Configuration for Vietnamese ASR model\"\"\"\n",
    "#     sample_rate: int = 16000\n",
    "#     n_mels: int = 80\n",
    "#     n_fft: int = 400\n",
    "#     hop_length: int = 160\n",
    "#     win_length: int = 400\n",
    "#     f_min: float = 0\n",
    "#     f_max: Optional[float] = 8000\n",
    "    \n",
    "#     # Model architecture\n",
    "#     encoder_dim: int = 144\n",
    "#     num_encoder_layers: int = 12\n",
    "#     num_attention_heads: int = 4\n",
    "#     feed_forward_expansion_factor: int = 4\n",
    "#     conv_expansion_factor: int = 2\n",
    "#     input_dropout_p: float = 0.1\n",
    "#     feed_forward_dropout_p: float = 0.1\n",
    "#     attention_dropout_p: float = 0.1\n",
    "#     conv_dropout_p: float = 0.1\n",
    "#     conv_kernel_size: int = 31\n",
    "    \n",
    "#     # Training\n",
    "#     batch_size: int = 32\n",
    "#     num_workers: int = 4\n",
    "#     learning_rate: float = 5e-4\n",
    "#     weight_decay: float = 1e-6\n",
    "#     max_epochs: int = 100\n",
    "    \n",
    "#     # Vocabulary\n",
    "#     vocab_size: int = 8000  # Adjust based on your tokenizer\n",
    "\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     \"\"\"Positional encoding for transformer\"\"\"\n",
    "#     def __init__(self, d_model: int, max_len: int = 5000):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "#                            (-np.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     \"\"\"Multi-head attention module\"\"\"\n",
    "#     def __init__(self, config: ASRConfig):\n",
    "#         super().__init__()\n",
    "#         self.d_model = config.encoder_dim\n",
    "#         self.num_heads = config.num_attention_heads\n",
    "#         self.dropout = nn.Dropout(config.attention_dropout_p)\n",
    "        \n",
    "#         self.q_linear = nn.Linear(config.encoder_dim, config.encoder_dim)\n",
    "#         self.k_linear = nn.Linear(config.encoder_dim, config.encoder_dim)\n",
    "#         self.v_linear = nn.Linear(config.encoder_dim, config.encoder_dim)\n",
    "#         self.out = nn.Linear(config.encoder_dim, config.encoder_dim)\n",
    "        \n",
    "#     def forward(self, query: torch.Tensor, key: torch.Tensor, \n",
    "#                 value: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "#         batch_size = query.size(0)\n",
    "        \n",
    "#         # Linear projections\n",
    "#         Q = self.q_linear(query)\n",
    "#         K = self.k_linear(key)\n",
    "#         V = self.v_linear(value)\n",
    "        \n",
    "#         # Split into heads\n",
    "#         Q = Q.view(batch_size, -1, self.num_heads, \n",
    "#                   self.d_model // self.num_heads).transpose(1, 2)\n",
    "#         K = K.view(batch_size, -1, self.num_heads, \n",
    "#                   self.d_model // self.num_heads).transpose(1, 2)\n",
    "#         V = V.view(batch_size, -1, self.num_heads, \n",
    "#                   self.d_model // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "#         # Scaled dot-product attention\n",
    "#         scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
    "        \n",
    "#         if mask is not None:\n",
    "#             scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "#         attention = F.softmax(scores, dim=-1)\n",
    "#         attention = self.dropout(attention)\n",
    "        \n",
    "#         # Apply attention to V\n",
    "#         out = torch.matmul(attention, V)\n",
    "        \n",
    "#         # Concatenate heads\n",
    "#         out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "#         return self.out(out)\n",
    "\n",
    "# class ConformerBlock(nn.Module):\n",
    "#     \"\"\"Conformer block implementation\"\"\"\n",
    "#     def __init__(self, config: ASRConfig):\n",
    "#         super().__init__()\n",
    "#         self.feed_forward_1 = nn.Sequential(\n",
    "#             nn.Linear(config.encoder_dim, config.encoder_dim * \n",
    "#                      config.feed_forward_expansion_factor),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config.feed_forward_dropout_p),\n",
    "#             nn.Linear(config.encoder_dim * config.feed_forward_expansion_factor, \n",
    "#                      config.encoder_dim)\n",
    "#         )\n",
    "        \n",
    "#         self.mha = MultiHeadAttention(config)\n",
    "        \n",
    "#         self.conv_module = nn.Sequential(\n",
    "#             nn.LayerNorm(config.encoder_dim),\n",
    "#             nn.Conv1d(config.encoder_dim, config.encoder_dim * \n",
    "#                      config.conv_expansion_factor, 1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(config.encoder_dim * config.conv_expansion_factor, \n",
    "#                      config.encoder_dim, config.conv_kernel_size, \n",
    "#                      padding=config.conv_kernel_size // 2, \n",
    "#                      groups=config.encoder_dim),\n",
    "#             nn.BatchNorm1d(config.encoder_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv1d(config.encoder_dim, config.encoder_dim, 1),\n",
    "#             nn.Dropout(config.conv_dropout_p)\n",
    "#         )\n",
    "        \n",
    "#         self.feed_forward_2 = nn.Sequential(\n",
    "#             nn.Linear(config.encoder_dim, config.encoder_dim * \n",
    "#                      config.feed_forward_expansion_factor),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(config.feed_forward_dropout_p),\n",
    "#             nn.Linear(config.encoder_dim * config.feed_forward_expansion_factor, \n",
    "#                      config.encoder_dim)\n",
    "#         )\n",
    "        \n",
    "#         self.layer_norm = nn.LayerNorm(config.encoder_dim)\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "#         # First Feed Forward\n",
    "#         ff1 = self.feed_forward_1(x)\n",
    "#         x = ff1 * 0.5 + x\n",
    "        \n",
    "#         # Multi-Head Attention\n",
    "#         att = self.mha(x, x, x, mask)\n",
    "#         x = att + x\n",
    "        \n",
    "#         # Convolution Module\n",
    "#         x = x.transpose(1, 2)\n",
    "#         conv = self.conv_module(x)\n",
    "#         x = conv + x\n",
    "#         x = x.transpose(1, 2)\n",
    "        \n",
    "#         # Second Feed Forward\n",
    "#         ff2 = self.feed_forward_2(x)\n",
    "#         x = ff2 * 0.5 + x\n",
    "        \n",
    "#         return self.layer_norm(x)\n",
    "\n",
    "# class VietnameseASR(nn.Module):\n",
    "#     \"\"\"Vietnamese ASR model implementation\"\"\"\n",
    "#     def __init__(self, config: ASRConfig):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "        \n",
    "#         # Front-end\n",
    "#         self.frontend = nn.Sequential(\n",
    "#             nn.Conv2d(1, config.encoder_dim, 3, stride=2, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(config.encoder_dim, config.encoder_dim, 3, stride=2, padding=1),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#         # Positional encoding\n",
    "#         self.pos_encoding = PositionalEncoding(config.encoder_dim)\n",
    "        \n",
    "#         # Conformer blocks\n",
    "#         self.conformer_blocks = nn.ModuleList([\n",
    "#             ConformerBlock(config) for _ in range(config.num_encoder_layers)\n",
    "#         ])\n",
    "        \n",
    "#         # Output layer\n",
    "#         self.output_layer = nn.Linear(config.encoder_dim, config.vocab_size)\n",
    "        \n",
    "#     def forward(self, x: torch.Tensor, \n",
    "#                 mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "#         # Front-end processing\n",
    "#         x = x.unsqueeze(1)  # Add channel dimension\n",
    "#         x = self.frontend(x)\n",
    "#         b, c, t, f = x.size()\n",
    "#         x = x.transpose(1, 2).contiguous().view(b, t, c * f)\n",
    "        \n",
    "#         # Add positional encoding\n",
    "#         x = self.pos_encoding(x)\n",
    "        \n",
    "#         # Conformer blocks\n",
    "#         for block in self.conformer_blocks:\n",
    "#             x = block(x, mask)\n",
    "        \n",
    "#         # Output layer\n",
    "#         x = self.output_layer(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model: nn.Module, train_loader: DataLoader, \n",
    "#                 val_loader: DataLoader, config: ASRConfig) -> nn.Module:\n",
    "#     \"\"\"Training function\"\"\"\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model = model.to(device)\n",
    "    \n",
    "#     optimizer = torch.optim.AdamW(\n",
    "#         model.parameters(),\n",
    "#         lr=config.learning_rate,\n",
    "#         weight_decay=config.weight_decay\n",
    "#     )\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "#         optimizer,\n",
    "#         max_lr=config.learning_rate,\n",
    "#         epochs=config.max_epochs,\n",
    "#         steps_per_epoch=len(train_loader)\n",
    "#     )\n",
    "    \n",
    "#     criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "    \n",
    "#     for epoch in range(config.max_epochs):\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "        \n",
    "#         for batch_idx, (specs, transcripts) in enumerate(train_loader):\n",
    "#             specs = specs.to(device)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             outputs = model(specs)\n",
    "            \n",
    "#             # Calculate loss (you'll need to implement proper CTC loss calculation)\n",
    "#             loss = criterion(outputs, transcripts)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "            \n",
    "#             train_loss += loss.item()\n",
    "            \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for specs, transcripts in val_loader:\n",
    "#                 specs = specs.to(device)\n",
    "#                 outputs = model(specs)\n",
    "#                 loss = criterion(outputs, transcripts)\n",
    "#                 val_loss += loss.item()\n",
    "        \n",
    "#         print(f\"Epoch {epoch+1}/{config.max_epochs}\")\n",
    "#         print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "#         print(f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# def quantize_model(model: nn.Module) -> nn.Module:\n",
    "#     \"\"\"Quantize model to INT8\"\"\"\n",
    "#     model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "#     torch.quantization.prepare(model, inplace=True)\n",
    "#     torch.quantization.convert(model, inplace=True)\n",
    "#     return model\n",
    "\n",
    "# def export_model(model: nn.Module, path: str):\n",
    "#     \"\"\"Export model to ONNX format\"\"\"\n",
    "#     dummy_input = torch.randn(1, 80, 500)  # Example input shape\n",
    "#     torch.onnx.export(model, dummy_input, path, \n",
    "#                      input_names=['input'], \n",
    "#                      output_names=['output'],\n",
    "#                      dynamic_axes={'input': {0: 'batch_size', 2: 'sequence_length'},\n",
    "#                                  'output': {0: 'batch_size', 1: 'sequence_length'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def beamSearch(predict, width):\n",
    "#     # Initialize the beam search\n",
    "#     beam = [(list(), 1.0)]\n",
    "    \n",
    "#     # Step through each prediction\n",
    "#     for i in range(len(predict)):\n",
    "#         next_beam = list()\n",
    "        \n",
    "#         # Expand each beam\n",
    "#         for prefix, prob in beam:\n",
    "#             if len(prefix) > 0:\n",
    "#                 last = prefix[-1]\n",
    "#             else:\n",
    "#                 last = None\n",
    "            \n",
    "#             # Add each token to the prefix\n",
    "#             for j, p in enumerate(predict[i]):\n",
    "#                 if last == j:\n",
    "#                     continue\n",
    "                \n",
    "#                 # Calculate the new probability\n",
    "#                 new_prob = prob * p\n",
    "                \n",
    "#                 # Add the new prefix to the next beam\n",
    "#                 next_beam.append((prefix + [j], new_prob))\n",
    "        \n",
    "#         # Sort the beam\n",
    "#         beam = sorted(next_beam, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "#         # Keep only the top `width` prefixes\n",
    "#         beam = beam[:width]\n",
    "    \n",
    "#     return beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.load('x_train.npy')\n",
    "# y_train = np.load('y_train.npy')\n",
    "# x_val = np.load('x_val.npy')\n",
    "# y_val = np.load('y_val.npy')\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(x_train-, batch_size=32, shuffle=True)\n",
    "# config = ASRConfig()\n",
    "# model = VietnameseASR(config)\n",
    "\n",
    "# trained_model = train_model(model, train_loader, val_loader, config)\n",
    "# quantized_model = quantize_model(trained_model)\n",
    "# export_model(quantized_model, 'asr_model.onnx')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
