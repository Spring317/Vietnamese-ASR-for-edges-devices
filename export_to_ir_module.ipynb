{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sQqHsKpMstmJ","outputId":"317c069c-c70e-48b4-e0db-4d48dfa26486"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/spring/miniconda3/lib/python3.12/site-packages/transformers/configuration_utils.py:306: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  warnings.warn(\n"]}],"source":["from transformers import Wav2Vec2Config\n","model_name = \"nguyenvulebinh/wav2vec2-base-vietnamese-250h\"\n","config = Wav2Vec2Config.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3otd7HW7stmL"},"outputs":[],"source":["import tvm\n","from tvm import dlight, relax, te, tir\n","from tvm.relax import register_pipeline\n","from tvm.relax.frontend import nn\n","from tvm.relax.frontend.nn import Tensor, op\n","from tvm.relax.frontend.nn.llm.kv_cache import PagedKVCache, TIRPagedKVCache\n","from tvm.runtime import ShapeTuple"]},{"cell_type":"markdown","metadata":{"id":"0v_M_fvWstmM"},"source":["## Linear layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rNyrTPV0stmP"},"outputs":[],"source":["class Wav2Vec2Linear(nn.Module):\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(Wav2Vec2Linear, self).__init__()\n","        self.linear = nn.Linear(in_features, out_features, bias)\n","    def forward(self, input_values):\n","        return self.linear(input_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3Wp9h3jstmQ"},"outputs":[],"source":["mod, params = Wav2Vec2Linear(768, 768).export_tvm({\"forward\": {\"input_values\": nn.spec.Tensor((768,768), \"float32\")}})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7BluJmvstmQ","outputId":"ff49d7a8-4b33-4268-a556-5c11f3792568"},"outputs":[{"data":{"text/html":["<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n","<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n","\n","<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n","<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n","    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n","    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">forward</span>(input_values: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), linear_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), linear_bias: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n","        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n","        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n","            matmul_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(linear_weight, axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n","            matmul: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(input_values, matmul_weight, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            add: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(matmul, linear_bias)\n","            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> add\n","            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n","        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n","</pre></div>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["mod.show()"]},{"cell_type":"markdown","metadata":{"id":"ieZVi7JtstmR"},"source":["## Feature Extractor Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekj6ESA9stmS","outputId":"6598d615-5892-47e6-c1c2-226a7d4cbd6c"},"outputs":[{"data":{"text/html":["<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n","<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n","\n","<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n","<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n","    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n","    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">forward</span>(input_values: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">86000</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_0_conv_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_0_layer_norm_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_0_layer_norm_bias: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_1_conv_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_2_conv_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_3_conv_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_4_conv_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">3</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_5_conv_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">2</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), conv_layers_6_conv_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">2</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">268</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n","        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n","        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n","            conv1d: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">17199</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv1d(input_values, conv_layers_0_conv_weight, strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">5</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            relu: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">17199</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(conv1d)\n","            group_norm: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">17199</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>group_norm(relu, conv_layers_0_layer_norm_weight, conv_layers_0_layer_norm_bias, num_groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">512</span>, channel_axis<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, axes<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>], epsilon<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1.0000000000000001e-05</span>, center<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>, scale<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n","            conv1d1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">8599</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv1d(group_norm, conv_layers_1_conv_weight, strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            relu1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">8599</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(conv1d1)\n","            conv1d2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">4299</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv1d(relu1, conv_layers_2_conv_weight, strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            relu2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">4299</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(conv1d2)\n","            conv1d3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">2149</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv1d(relu2, conv_layers_3_conv_weight, strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            relu3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">2149</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(conv1d3)\n","            conv1d4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">1074</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv1d(relu3, conv_layers_4_conv_weight, strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            relu4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">1074</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(conv1d4)\n","            conv1d5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">537</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv1d(relu4, conv_layers_5_conv_weight, strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            relu5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">537</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(conv1d5)\n","            conv1d6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">268</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>conv1d(relu5, conv_layers_6_conv_weight, strides<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">2</span>], padding<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">0</span>, <span style=\"color: #008000\">0</span>], dilation<span style=\"color: #AA22FF; font-weight: bold\">=</span>[<span style=\"color: #008000\">1</span>], groups<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000\">1</span>, data_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, kernel_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;OIW&quot;</span>, out_layout<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;NCW&quot;</span>, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            relu6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">268</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(conv1d6)\n","            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">268</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> relu6\n","            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n","        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n","</pre></div>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# import tvm\n","# from tvm import dlight, relax, te, tir\n","# from tvm.relax import register_pipeline\n","# from tvm.relax.frontend import nn\n","# from tvm.relax.frontend.nn import Tensor, op\n","# from tvm.relax.frontend.nn.llm.kv_cache import PagedKVCache, TIRPagedKVCache\n","# from tvm.runtime import ShapeTuple\n","\n","class Wav2VecNoLayerNormConvLayer(nn.Module):\n","\n","    def __init__(self, config, layer_id = 0):\n","        super().__init__()\n","        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n","        self.out_conv_dim = config.conv_dim[layer_id]\n","\n","        self.conv = nn.Conv1D(\n","            self.in_conv_dim,\n","            self.out_conv_dim,\n","            kernel_size=config.conv_kernel[layer_id],\n","            stride=config.conv_stride[layer_id],\n","            bias= config.conv_bias,\n","        )\n","\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.conv(hidden_states)\n","        hidden_states = self.activation(hidden_states)\n","        return hidden_states\n","\n","\n","class Wav2Vec2GroupNormConvLayer(nn.Module):\n","\n","    def __init__(self, config, layer_id = 0):\n","        super().__init__()\n","        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n","        self.out_conv_dim = config.conv_dim[layer_id]\n","\n","        self.conv = nn.Conv1D(\n","            self.in_conv_dim,\n","            self.out_conv_dim,\n","            kernel_size=config.conv_kernel[layer_id],\n","            stride=config.conv_stride[layer_id],\n","            bias= config.conv_bias,\n","        )\n","\n","        self.activation = nn.ReLU()\n","        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n","\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.conv(hidden_states)\n","        hidden_states = self.activation(hidden_states)\n","        hidden_states =self.layer_norm(hidden_states)\n","        return hidden_states\n","\n","\n","class Wav2Vec2FeatureEncoder(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        conv_layers = [Wav2Vec2GroupNormConvLayer(config, layer_id = 0)] + [Wav2VecNoLayerNormConvLayer(config, layer_id = i+1) for i in range(config.num_feat_extract_layers -1)]\n","        self.conv_layers = nn.ModuleList(conv_layers)\n","\n","    def forward(self, input_values):\n","        hidden_states = input_values\n","        for conv_layer in self.conv_layers:\n","            hidden_states = conv_layer(hidden_states)\n","        return hidden_states\n","\n","mod, params = Wav2Vec2FeatureEncoder(config).export_tvm({\"forward\": {\"input_values\": nn.spec.Tensor((1, 1, 86000), \"float32\")}})\n","mod.show()"]},{"cell_type":"markdown","metadata":{"id":"x6ZagwaastmT"},"source":["## Feature Projection layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iO7AUduFstmT","outputId":"1d2efc25-f903-4040-adfa-0ca30731ec59"},"outputs":[{"data":{"text/html":["<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n","<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n","\n","<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n","<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n","    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n","    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">forward</span>(hidden_states: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">268</span>, <span style=\"color: #008000\">512</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), layer_norm_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), layer_norm_bias: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), projection_weight: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>, <span style=\"color: #008000\">512</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), projection_bias: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">768</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">268</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n","        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;num_input&quot;</span>: <span style=\"color: #008000\">1</span>})\n","        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n","            permute_dims: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(projection_weight, axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n","            matmul: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">268</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(hidden_states, permute_dims, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n","            add: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">268</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(matmul, projection_bias)\n","            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">268</span>, <span style=\"color: #008000\">768</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> add\n","            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n","        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n","</pre></div>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["class Wav2Vec2FeatureProjection(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)\n","        self.projection = nn.Linear(config.conv_dim[-1], config.hidden_size)\n","        # self.dropout = nn.Dropout(config.proj_dropout)\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.projection(hidden_states)\n","        # hidden_states = self.dropout(hidden_states)\n","        return hidden_states\n","\n","mod, params = Wav2Vec2FeatureProjection(config).export_tvm({\"forward\": {\"hidden_states\": nn.spec.Tensor((1, 268, 512), \"float32\")}})\n","mod.show()"]},{"cell_type":"markdown","metadata":{"id":"e79a0xnVstmU"},"source":["## Attention is all you need"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2TJKgABstmU"},"outputs":[],"source":["from typing import Optional, Tuple\n","class Wav2Vec2Attention(nn.Module):\n","    def __init__(\n","            self,\n","            embed_dim: int,\n","            num_heads: int,\n","            dropout: float = 0.0,\n","            is_decoder: bool = False,\n","            bias: bool = True,\n","            is_causal: bool = False,\n","            config: Optional[Wav2Vec2Config] = None,\n","    ):\n","        super().__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        self.dropout = dropout\n","        self.head_dim = embed_dim // num_heads\n","        self.config = config\n","\n","        if (self.head_dim * num_heads) != embed_dim:\n","            raise ValueError(\n","                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {embed_dim}, `num_heads`: {num_heads})\"\n","            )\n","\n","        self.scaling = self.head_dim ** -0.5\n","        self.is_decoder = is_decoder\n","        self.is_causal = is_causal\n","\n","        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n","\n","    def _shape(self, x, seq_len, bsz: int):\n","        x =  op.reshape(x, (bsz, seq_len, self.num_heads, self.head_dim))\n","        x = op.permute_dims(x, [0, 2, 1, 3])\n","\n","        return x\n","\n","    def forward(\n","           self,\n","            hidden_states: Tensor,\n","            key_value_states: Optional[Tensor] = None,\n","            past_key_value: Optional[Tuple[Tensor]] = None,\n","            attention_mask: Optional[Tensor] = None,\n","            layer_head_mask: Optional[Tensor] = None,\n","            output_attentions: bool = False,\n","\n","    )-> Tuple[Tensor, Optional[Tensor], Optional[Tuple]]:\n","        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n","        is_cross_attention = key_value_states is not None\n","\n","        bsz, tgt_len, _ = hidden_states.shape\n","        query_states = self.q_proj(hidden_states) * self.scaling\n","        # get key, value proj\n","        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n","        # is checking that the `sequence_length` of the `past_key_value` is the same as\n","        # the provided `key_value_states` to support prefix tuning\n","        if (\n","            is_cross_attention\n","            and past_key_value is not None\n","            and past_key_value[0].shape[2] == key_value_states.shape[1]\n","        ):\n","            # reuse k,v, cross_attentions\n","            key_states = past_key_value[0]\n","            value_states = past_key_value[1]\n","        elif is_cross_attention:\n","            # cross_attentions\n","            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n","        elif past_key_value is not None:\n","            # reuse k, v, self_attention\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","            # key_states = cat([past_key_value[0], key_states], dim=2)\n","            # value_states = cat([past_key_value[1], value_states], dim=2)\n","            key_states = op.concatenate([past_key_value[0], key_states], axis=2)\n","            value_states = op.concatenate([past_key_value[1], value_states], axis=2)\n","\n","        else:\n","            # self_attention\n","            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n","            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n","\n","        if self.is_decoder:\n","            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n","            # Further calls to cross_attention layer can then reuse all cross-attention\n","            # key/value_states (first \"if\" case)\n","            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n","            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n","            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n","            # if encoder bi-directional self-attention `past_key_value` is always `None`\n","            past_key_value = (key_states, value_states)\n","\n","        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n","        query_states = self._shape(query_states, tgt_len, bsz).reshape(*proj_shape)\n","        key_states = key_states.reshape(*proj_shape)\n","        value_states = value_states.reshape(*proj_shape)\n","        print(f\"key_states shape: {key_states.shape}\")\n","        src_len = key_states.shape[1]\n","\n","        # attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n","        attn_weights = op.matmul(query_states, op.permute_dims(key_states, [0, 2, 1]))\n","        print(type(attn_weights))\n","        if attn_weights.shape != (bsz * self.num_heads, tgt_len, src_len):\n","            # raise ValueError(\n","            #     f\"Attention weights should be of shape {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n","            #     f\" {attn_weights.shape}\"\n","            # )\n","            pass\n","\n","        if attention_mask is not None:\n","            if attention_mask.shape() != (bsz, 1, tgt_len, src_len):\n","                raise ValueError(\n","                    f\"Attention mask should be of shape {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.shape()}\"\n","                )\n","            attn_weights = attn_weights.reshape(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n","            attn_weights = attn_weights.reshape(bsz * self.num_heads, tgt_len, src_len)\n","\n","        attn_weights = nn.softmax(attn_weights,axis=-1)\n","\n","        if layer_head_mask is not None:\n","            if layer_head_mask.shape() != (self.num_heads,):\n","                raise ValueError(\n","                    f\"Head mask for a single layer should be of shape {(self.num_heads,)}, but is\"\n","                    f\" {layer_head_mask.shape()}\"\n","                )\n","            attn_weights = layer_head_mask.reshape(1, -1, 1, 1) * attn_weights.reshape(bsz, self.num_heads, tgt_len, src_len)\n","            attn_weights = attn_weights.reshape(bsz * self.num_heads, tgt_len, src_len)\n","\n","        if output_attentions:\n","            # this operation is a bit awkward, but it's required to\n","            # make sure that attn_weights keeps its gradient.\n","            # In order to do so, attn_weights have to be reshaped\n","            # twice and have to be reused in the following\n","            attn_weights_reshaped = attn_weights.reshape(bsz, self.num_heads, tgt_len, src_len)\n","            attn_weights = attn_weights_reshaped.reshape(bsz * self.num_heads, tgt_len, src_len)\n","        else:\n","            attn_weights_reshaped = None\n","\n","        # attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n","        attn_probs = attn_weights\n","\n","        # attn_output = torch.bmm(attn_probs, value_states)\n","        attn_output = op.matmul(attn_probs, value_states)\n","\n","        # if attn_output.shape() != (bsz * self.num_heads, tgt_len, self.head_dim):\n","        #     raise ValueError(\n","        #         f\"`attn_output` should be of shape {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n","        #         f\" {attn_output.shape()}\"\n","        #     )\n","\n","        # attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n","        # attn_output = attn_output.transpose(1, 2)\n","        attn_output = op.reshape(attn_output, (bsz, self.num_heads, tgt_len, self.head_dim))\n","        # attn_output = op.transpose(attn_output, (0, 2, 1, 3))\n","        attn_output = op.permute_dims(attn_output, [0, 2, 1, 3])\n","\n","        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n","        # partitioned across GPUs when using tensor-parallelism.\n","        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n","\n","        attn_output = self.out_proj(attn_output)\n","\n","        return attn_output, attn_weights_reshaped, past_key_value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xg5SQpQystmW","outputId":"4e94f4c1-8f35-4d9b-fe0e-2f00ee6fdcd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["key_states shape: [8, 268, 64]\n","<class 'tvm.relax.frontend.nn.core.Tensor'>\n"]},{"name":"stderr","output_type":"stream","text":["[14:55:50] /home/spring/tvm/src/relax/ir/block_builder.cc:65: Warning: BlockBuilder destroyed with remaining blocks!\n"]},{"ename":"TypeError","evalue":"Unsupported return type: <class 'NoneType'>","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mod, params \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2Attention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_tvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_states\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m268\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m mod\u001b[38;5;241m.\u001b[39mshow()\n","File \u001b[0;32m~/tvm/python/tvm/relax/frontend/nn/core.py:489\u001b[0m, in \u001b[0;36mModule.export_tvm\u001b[0;34m(self, spec, debug, allow_extern)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# pylint: enable=import-outside-toplevel\u001b[39;00m\n\u001b[1;32m    488\u001b[0m spec \u001b[38;5;241m=\u001b[39m _spec\u001b[38;5;241m.\u001b[39mModuleSpec\u001b[38;5;241m.\u001b[39mfrom_raw(spec, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 489\u001b[0m mod, params, ext_mods \u001b[38;5;241m=\u001b[39m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_extern:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mod, params, ext_mods\n","File \u001b[0;32m~/tvm/python/tvm/relax/frontend/nn/exporter.py:136\u001b[0m, in \u001b[0;36mExporter.build\u001b[0;34m(self, spec)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m    132\u001b[0m             method_name,\n\u001b[1;32m    133\u001b[0m             attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: len_args \u001b[38;5;241m+\u001b[39m len_effects},  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    134\u001b[0m         ):\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mdataflow():\n\u001b[0;32m--> 136\u001b[0m                 outputs, inputs \u001b[38;5;241m=\u001b[39m \u001b[43m_emit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39memit_func_output(outputs, inputs)\n\u001b[1;32m    138\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mfinalize()\n","File \u001b[0;32m~/tvm/python/tvm/relax/frontend/nn/exporter.py:286\u001b[0m, in \u001b[0;36m_emit_method\u001b[0;34m(builder, spec, params, effects)\u001b[0m\n\u001b[1;32m    284\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39memit_output(rx\u001b[38;5;241m.\u001b[39mTuple([_unwrap_ret(outputs), rx\u001b[38;5;241m.\u001b[39mTuple(effect_outputs)]))\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39memit_output(\u001b[43m_unwrap_ret\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, inputs\n","File \u001b[0;32m~/tvm/python/tvm/relax/frontend/nn/exporter.py:171\u001b[0m, in \u001b[0;36m_emit_method.<locals>._unwrap_ret\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\u001b[38;5;241m.\u001b[39m_expr\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rx\u001b[38;5;241m.\u001b[39mTuple([\u001b[43m_unwrap_ret\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m expr])\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rx\u001b[38;5;241m.\u001b[39mTuple([_unwrap_ret(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m expr])\n","File \u001b[0;32m~/tvm/python/tvm/relax/frontend/nn/exporter.py:174\u001b[0m, in \u001b[0;36m_emit_method.<locals>._unwrap_ret\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rx\u001b[38;5;241m.\u001b[39mTuple([_unwrap_ret(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m expr])\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported return type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(expr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: Unsupported return type: <class 'NoneType'>"]}],"source":["mod, params = Wav2Vec2Attention(512, 8).export_tvm({\"forward\": {\"hidden_states\": nn.spec.Tensor((1, 268, 512), \"float32\")}})\n","mod.show()"]},{"cell_type":"markdown","metadata":{"id":"o-hgLczYstmW"},"source":["## Encoder layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrwUBZBpstmX"},"outputs":[],"source":["class Wav2Vec2EncoderLayer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.attention = nn.MultiHeadAttention(\n","            embed_dim=config.hidden_size,\n","            num_heads=config.num_attention_heads,\n","            dropout=config.attention_dropout,\n","        )\n","        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(config.hidden_size, config.intermediate_size),\n","            nn.GELU(),\n","            nn.Linear(config.intermediate_size, config.hidden_size),\n","        )\n","        self.dropout = nn.Dropout(config.hidden_dropout)"]},{"cell_type":"markdown","metadata":{"id":"_bZH97lSstmX"},"source":["## Compiling to DPU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8Izn3ejstmY"},"outputs":[],"source":["import pyxir\n","import pyxir.contrib.target.DPUCADF8H\n","\n","import tvm\n","import tvm.relay as relay\n","from tvm.contrib.target import vitis_ai\n","from tvm.contrib import utils, graph_executor\n","from tvm.relay.op.contrib.vitis_ai import partition_for_vitis_ai\n","\n","tvm_target = 'llvm'\n","dpu_target = 'DPUCZDX8G-kv260' # options: 'DPUCADF8H', 'DPUCAHX8H-u50', 'DPUCAHX8H-u280', 'DPUCAHX8L', 'DPUCVDX8H', 'DPUCZDX8G-zcu104', 'DPUCZDX8G-zcu102', 'DPUCZDX8G-kv260'"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}